{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_loaded = load_dotenv('../.envrc')\n",
    "assert env_loaded, 'Failed to load .envrc'\n",
    "\n",
    "DB_HOST = os.getenv('DB_HOST')\n",
    "assert DB_HOST is not None\n",
    "DB_PORT = os.getenv('DB_PORT')\n",
    "assert DB_PORT is not None\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "assert DB_USER is not None\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "assert DB_PASSWORD is not None\n",
    "DB_NAME = os.getenv('DB_NAME')\n",
    "assert DB_NAME is not None\n",
    "\n",
    "DB_URL = f'postgresql+asyncpg://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLLAMA_API_BASE: http://ollama:11434\n",
      "LITELLM_LLM_RERANKER_MODEL_NAME: ollama_chat/llama3.2:3b\n",
      "LLM_RERANKER_TOP_N: 4\n",
      "LLM_RERANKER_CHOICE_BATCH_SIZE: 5\n",
      "LITELLM_RESPONSE_SYNTHESIZER_MODEL: ollama_chat/llama3.2:1b\n",
      "SIMILARITY_TOP_K: 6\n",
      "SIMILARITY_CUTOFF: 0.8\n",
      "JINA_RERANKER_MODEL: jina-reranker-v2-base-multilingual\n",
      "JINA_RERANKER_TOP_N: 2\n",
      "LITELLM_CHAT_ENGINE_LLM_MODEL_NAME: ollama_chat/llama3.2:1b\n",
      "SENTENCE_TRANSFORMER_RERANKER_MODEL: cross-encoder/stsb-distilroberta-base\n",
      "SENTENCE_TRANSFORMER_RERANKER_TOP_N: 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "OLLAMA_API_BASE = os.getenv(\n",
    "    'OLLAMA_API_BASE',\n",
    ")\n",
    "print(f'OLLAMA_API_BASE: {OLLAMA_API_BASE}')\n",
    "\n",
    "LITELLM_LLM_RERANKER_MODEL_NAME = os.getenv(\n",
    "    'LITELLM_LLM_RERANKER_MODEL'\n",
    ")\n",
    "print(f'LITELLM_LLM_RERANKER_MODEL_NAME: {LITELLM_LLM_RERANKER_MODEL_NAME}')\n",
    "\n",
    "LLM_RERANKER_TOP_N = os.getenv(\n",
    "    'LLM_RERANKER_TOP_N',\n",
    ")\n",
    "print(f'LLM_RERANKER_TOP_N: {LLM_RERANKER_TOP_N}')\n",
    "\n",
    "LLM_RERANKER_CHOICE_BATCH_SIZE = os.getenv(\n",
    "    'LLM_RERANKER_CHOICE_BATCH_SIZE',\n",
    ")\n",
    "print(f'LLM_RERANKER_CHOICE_BATCH_SIZE: {LLM_RERANKER_CHOICE_BATCH_SIZE}')\n",
    "\n",
    "LITELLM_RESPONSE_SYNTHESIZER_MODEL = os.getenv(\n",
    "    'LITELLM_RESPONSE_SYNTHESIZER_MODEL',\n",
    ")\n",
    "print(f'LITELLM_RESPONSE_SYNTHESIZER_MODEL: {LITELLM_RESPONSE_SYNTHESIZER_MODEL}')\n",
    "\n",
    "SIMILARITY_TOP_K = os.getenv(\n",
    "    'SIMILARITY_TOP_K',\n",
    ")\n",
    "print(f'SIMILARITY_TOP_K: {SIMILARITY_TOP_K}')\n",
    "\n",
    "SIMILARITY_CUTOFF = os.getenv(\n",
    "    'SIMILARITY_CUTOFF',\n",
    ")\n",
    "print(f'SIMILARITY_CUTOFF: {SIMILARITY_CUTOFF}')\n",
    "\n",
    "JINA_RERANKER_MODEL = os.getenv(\n",
    "    'JINA_RERANKER_MODEL',\n",
    ")\n",
    "print(f'JINA_RERANKER_MODEL: {JINA_RERANKER_MODEL}')\n",
    "\n",
    "JINA_RERANKER_TOP_N = os.getenv(\n",
    "    'JINA_RERANKER_TOP_N',\n",
    ")\n",
    "print(f'JINA_RERANKER_TOP_N: {JINA_RERANKER_TOP_N}')\n",
    "\n",
    "LITELLM_CHAT_ENGINE_LLM_MODEL_NAME = os.getenv(\n",
    "    'LITELLM_CHAT_ENGINE_LLM_MODEL_NAME',\n",
    ")\n",
    "print(f'LITELLM_CHAT_ENGINE_LLM_MODEL_NAME: {LITELLM_CHAT_ENGINE_LLM_MODEL_NAME}')\n",
    "\n",
    "SENTENCE_TRANSFORMER_RERANKER_MODEL = os.getenv(\n",
    "    'SENTENCE_TRANSFORMER_RERANKER_MODEL',\n",
    ")\n",
    "print(f'SENTENCE_TRANSFORMER_RERANKER_MODEL: {SENTENCE_TRANSFORMER_RERANKER_MODEL}')\n",
    "\n",
    "SENTENCE_TRANSFORMER_RERANKER_TOP_N = os.getenv(\n",
    "    'SENTENCE_TRANSFORMER_RERANKER_TOP_N',\n",
    ")\n",
    "print(f'SENTENCE_TRANSFORMER_RERANKER_TOP_N: {SENTENCE_TRANSFORMER_RERANKER_TOP_N}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observability with Arize Phoenix\n",
    "- Check latency at each step\n",
    "- Check input and output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "import llama_index.core\n",
    "\n",
    "def launch_phoenix():\n",
    "    px.launch_app()\n",
    "    llama_index.core.set_global_handler(\"arize_phoenix\")\n",
    "\n",
    "def close_phoenix():\n",
    "    px.close_app()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "launch_phoenix() # observability tool into the chat engine e.g. retrieval, reranking, response generation etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import re\n",
    "\n",
    "embed_model_name = \"intfloat/multilingual-e5-large\"\n",
    "embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "embedding_model_dimensions = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table_name: budget_2025-intfloat-multilingual-e5-large\n"
     ]
    }
   ],
   "source": [
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "table_prefix = 'budget_2025-'\n",
    "model_name_clean = re.sub(r'[^a-zA-Z0-9\\-]', '-', embed_model_name)\n",
    "table_name = f'{table_prefix}{model_name_clean}'\n",
    "print(f'table_name: {table_name}')\n",
    "\n",
    "vector_store = PGVectorStore.from_params(\n",
    "            host=DB_HOST,\n",
    "            port=DB_PORT,\n",
    "            database=DB_NAME,\n",
    "            user=DB_USER,\n",
    "            password=DB_PASSWORD,\n",
    "            table_name=table_name,\n",
    "            perform_setup=False,\n",
    "            embed_dim=embedding_model_dimensions,\n",
    "        )\n",
    "\n",
    "vsi = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"as an undergraduate, what benefits do i get?\"\n",
    "question = \"ÊàëÊòØ‰∏Ä‰∏™Â§ßÂ≠¶Áîü ÊîøÂ∫úÊúâ‰ªÄ‰πàË°•Ë¥¥ÂêóÔºü\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vsi.as_retriever(similarity_top_k=SIMILARITY_TOP_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert vsi._embed_model.model_name == embed_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8467587919975063\n",
      "## 12Êúà\n",
      "\n",
      "Áé∞ÈáëË°•Âä© $\\longrightarrow 100$ ÂÖÉËá≥600ÂÖÉ\n",
      "[ÂÆöÂøÉ‰∏éÊé•Âä©ÈÖçÂ•ó]\n"
     ]
    }
   ],
   "source": [
    "nodes = retriever.retrieve(question)\n",
    "print(nodes[0].score)\n",
    "print(nodes[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocessors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Similarity Cutoff Postprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity postprocessor\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "similarity_postprocessor = SimilarityPostprocessor(similarity_cutoff=SIMILARITY_CUTOFF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "## 12Êúà\n",
      "\n",
      "Áé∞ÈáëË°•Âä© $\\longrightarrow 100$ ÂÖÉËá≥600ÂÖÉ\n",
      "[ÂÆöÂøÉ‰∏éÊé•Âä©ÈÖçÂ•ó]\n"
     ]
    }
   ],
   "source": [
    "# Testing it out \n",
    "similarity_postprocessor_nodes = similarity_postprocessor.postprocess_nodes(nodes)\n",
    "print(len(similarity_postprocessor_nodes))\n",
    "print(similarity_postprocessor_nodes[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentence Transformer Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a0427b7d00c48178baf9a4aad01c07e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/607 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb08024026f046ff9c7e6eb33a614a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sentence transformer reranker postprocessor\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "sentence_transformer_reranker = SentenceTransformerRerank(\n",
    "    model=SENTENCE_TRANSFORMER_RERANKER_MODEL, top_n=SENTENCE_TRANSFORMER_RERANKER_TOP_N\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## ÊîØÊåÅÂä≥Âä®ÂäõËΩ¨Âûã\n",
      "\n",
      "- Êé®Âá∫Êñ∞ÁöÑÊäÄËÉΩÂàõÂâçÁ®ãÂä≥Âä®ÂäõÂèëÂ±ïÊ¥•Ë¥¥ÔºåÂåÖÊã¨‰∏∫ÈáçÊñ∞ËÆæËÆ°Â∑•‰ΩúÁöÑ‰ºÅ‰∏öÊèê‰æõÈ´òËææ $70 \\%$ ÁöÑËµÑÂä©\n",
      "- ÁªèÈáçÊñ∞ËÆæËÆ°ÁöÑÊäÄËÉΩÂàõÂâçÁ®ã‰ºÅ‰∏öË°•Âä©Â∞Ü‰ªé2026Âπ¥‰∏ãÂçäÂπ¥Ëµ∑ÔºåÁªô‰∫àÁ¨¶ÂêàÊù°‰ª∂ÁöÑ‰ºÅ‰∏ö 1 ‰∏áÂÖÉÔºåËÆ©ÂÆÉ‰ª¨ÊäµÊ∂àÊé®Ë°åÂêàÊ†ºÂä≥Âä®ÂäõËΩ¨ÂûãËÆ°ÂàíÁöÑËá™‰ªòÂºÄÈîÄ\n",
      "- ‰∏∫ÂÖ®ÂõΩËÅåÂ∑•ÊÄª‰ºö‰ºÅ‰∏öÂüπËÆ≠ÂßîÂëò‰ºöË°•Âä©ÈáëÈ¢ùÂ§ñÊã®Ê¨æ 2 ‰∫øÂÖÉÔºå‰ª•ÂçèÂä©Êõ¥Â§ö‰ºÅ‰∏öËΩ¨ÂûãÂíåÊèêÂçáÂëòÂ∑•ÊäÄËÉΩ\n"
     ]
    }
   ],
   "source": [
    "# Testing out\n",
    "reranked_nodes = sentence_transformer_reranker.postprocess_nodes(nodes, query_str=question)\n",
    "print(reranked_nodes[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Jina Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor.jinaai_rerank import JinaRerank\n",
    "\n",
    "JINA_API_KEY = os.getenv('JINA_API_KEY')\n",
    "assert JINA_API_KEY is not None\n",
    "\n",
    "jina_reranker = JinaRerank(\n",
    "    top_n=JINA_RERANKER_TOP_N, model=JINA_RERANKER_MODEL, api_key=JINA_API_KEY\n",
    ")\n",
    "\n",
    "# Testing\n",
    "# reranked_nodes = postprocessor.postprocess_nodes(nodes, query_str=question)\n",
    "# len(reranked_nodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LLM Reranker\n",
    "Not used as it is too slow though the post processed nodes are quite relevant to be passed to the LLM for generating response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Reranker\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "from llama_index.llms.litellm import LiteLLM\n",
    "\n",
    "\n",
    "llm_reranker_model_name = \"ollama_chat/llama3.2:3b\"\n",
    "llm_reranker_model = LiteLLM(llm_reranker_model_name)\n",
    "\n",
    "llm_reranker_top_n = 4\n",
    "choice_batch_size = 10\n",
    "\n",
    "llm_reranker = LLMRerank(llm=llm_reranker_model, top_n=llm_reranker_top_n, choice_batch_size=choice_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "reranked_nodes_llm_reranker = llm_reranker.postprocess_nodes(similarity_postprocessor_nodes, query_str=question)\n",
    "print(len(reranked_nodes_llm_reranker))\n",
    "print(reranked_nodes_llm_reranker[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response Synthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.core.response_synthesizers.type import ResponseMode\n",
    "from llama_index.llms.litellm import LiteLLM\n",
    "\n",
    "response_synthesizer_llm = LiteLLM(LITELLM_RESPONSE_SYNTHESIZER_MODEL)\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(llm=response_synthesizer_llm, response_mode=ResponseMode.COMPACT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÊîøÂ∫ú‰∏∫Âπ¥ËΩª‰∫∫Êèê‰æõÁöÑË°•Ë¥¥ÂåÖÊã¨Áé∞ÈáëË°•Âä©ÂíåÊïôËÇ≤ÂÇ®ËìÑÊà∑Â§¥Â°´Ë°•„ÄÇ\n"
     ]
    }
   ],
   "source": [
    "# testing out\n",
    "response_synthesizer_response = await response_synthesizer.asynthesize(query=question, nodes=reranked_nodes)\n",
    "print(response_synthesizer_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama_chat/llama3.2:1b context window: 2048\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.litellm import LiteLLM\n",
    "\n",
    "llm = LiteLLM(LITELLM_CHAT_ENGINE_LLM_MODEL_NAME)\n",
    "print(f'{LITELLM_CHAT_ENGINE_LLM_MODEL_NAME} context window: {llm.metadata.context_window}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat history: [ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='tell me more about SG culture pass')]), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='The SG Culture Pass is a program that offers cultural experiences and discounts to Singaporeans aged 18 and above, providing access to various institutions, events, and attractions.')]), ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='What are the key beenfits for undergraduate students? Are there housing subsidies provided for first time home buyers?')]), ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='What are the key beenfits for undergraduate students? Are there housing subsidies provided for first time home buyers?')]), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text=\"Unfortunately, I couldn't find any specific information about benefits or housing subsidies for undergraduate students with the SG Culture Pass. However, it's clear that the pass provides credits for arts and heritage activities, which may be of interest to students.\")])]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.storage.chat_store.postgres import PostgresChatStore\n",
    "\n",
    "chat_store = PostgresChatStore.from_uri(DB_URL)\n",
    "\n",
    "thread_id = 'thread_6'\n",
    "\n",
    "chat_history = chat_store.get_messages(thread_id)\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(\n",
    "    chat_store=chat_store,\n",
    "    chat_store_key=thread_id,\n",
    "    chat_history=chat_history,\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "print(f'chat history: {chat_history}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.chat_engine.types import ChatMode\n",
    "\n",
    "chat_engine_llm = LiteLLM(LITELLM_CHAT_ENGINE_LLM_MODEL_NAME)\n",
    "\n",
    "chat_engine = vsi.as_chat_engine(\n",
    "    chat_mode=ChatMode.BEST, \n",
    "    llm=chat_engine_llm,\n",
    "    similarity_top_k=SIMILARITY_TOP_K,\n",
    "    # node_postprocessors=[similarity_postprocessor, sentence_transformer_reranker], \n",
    "    node_postprocessors=[similarity_postprocessor, jina_reranker],\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    streaming=True,\n",
    "    # memory=memory, # remove memory for now as it does not work well with the chat engine i.e. the llm pays more attention to the memory than the context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama_index.core.agent.react.base.ReActAgent"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chat_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ollama_chat/llama3.2:3b'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_engine_llm._get_model_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Undergraduate students in Singapore can benefit from a $\\$ 500$ top-up to their Edusave account or Post-Secondary Education Account (PSEA) this year, which can help with education expenses. For first-time homebuyers, particularly young married couples and parents with young children, there are subsidies available through the Fresh Start scheme, which allows them to buy shorter-lease subsidised flats."
     ]
    }
   ],
   "source": [
    "from llama_index.core.chat_engine.types import StreamingAgentChatResponse\n",
    "\n",
    "# question = \"tell me more about SG culture pass\"\n",
    "question = \"What are the key beenfits for undergraduate students? Are there housing subsidies provided for first time home buyers?\"\n",
    "\n",
    "response: StreamingAgentChatResponse = chat_engine.stream_chat(question)\n",
    "for token in response.response_gen:\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent with Tools\n",
    "Tools:\n",
    "- Search from knowledge base (QueryEngineTool)\n",
    "- Search from web (FunctionTool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Query Engine Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LiteLLM(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7f10ba05ddc0>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x7f118598a340>, completion_to_prompt=<function default_completion_to_prompt at 0x7f1185751260>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model='ollama_chat/llama3.2:3b', temperature=0.1, max_tokens=None, additional_kwargs={}, max_retries=10)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_synthesizer._llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "query_engine = vsi.as_query_engine(\n",
    "    llm=response_synthesizer._llm,\n",
    "    similarity_top_k=SIMILARITY_TOP_K,\n",
    "    node_postprocessors=[similarity_postprocessor, sentence_transformer_reranker],\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "search_knowledge_base_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine,\n",
    "    name='search_knowledge_base',\n",
    "    description=\"Search information from the knowledge base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web Search Tool\n",
    "Web search just in case the answer is not retrieved from the knowledge base? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAVILY_API_KEY = os.getenv('TAVILY_API_KEY')\n",
    "assert TAVILY_API_KEY is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from tavily import AsyncTavilyClient\n",
    "\n",
    "async def search_web(query: str) -> str:\n",
    "    \"\"\"Useful for using the web to answer questions.\"\"\"\n",
    "    client = AsyncTavilyClient(api_key=TAVILY_API_KEY)\n",
    "    return str(await client.search(query))\n",
    "\n",
    "\n",
    "tool = FunctionTool.from_defaults(\n",
    "    search_web,\n",
    "    name='search_web',\n",
    "    description=\"Useful for using the web to answer questions\"\n",
    "    # async_fn=asearch_web,  # optional!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import AsyncTavilyClient\n",
    "\n",
    "client = AsyncTavilyClient(api_key=TAVILY_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how much CDC vouchers can I get?\"\n",
    "\n",
    "include_domains = ['https://www.mof.gov.sg/singaporebudget']\n",
    "\n",
    "search_res = await client.search(query=query, include_domains=include_domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'how much CDC vouchers can I get?',\n",
       " 'follow_up_questions': None,\n",
       " 'answer': None,\n",
       " 'images': [],\n",
       " 'results': [{'title': 'Budget | Support For Singaporeans',\n",
       "   'url': 'https://www.mof.gov.sg/singaporebudget/budget-2025-highlights/support-for-singaporeans',\n",
       "   'content': 'Budget Speech Budget Resources Budget 2025 Highlights About Budget Budget 2025 Highlights CDC Vouchers [New]  $500 SG60 ActiveSG Credit Top-Up [New]   $100 SG60 Vouchers [New] $600 or $800 Child LifeSG Credits or Edusave Account / Post-Secondary Education Account Top-up [New] $500 MediSave [GSTV] $150 to $450 Large Family LifeSG Credits [New] SG Culture Pass [New]   $100 Personal Income Tax Rebate for Year of Assessment (YA) 2025 [New]   Up to $200 CDC Vouchers [New]  $300 Support for You and Your Households Singapore Budget 2025 is part of the Ministry of Finance, Singapore. Singapore Budget 2025 Budget Speech Budget Statement Budget Resources Budget 2025 Highlights Support For You And Your Household Support For Vulnerable Families And Persons With Disabilities About Budget Budget Archives',\n",
       "   'score': 0.78846955,\n",
       "   'raw_content': None},\n",
       "  {'title': 'Revenue and Expenditure - Budget',\n",
       "   'url': 'https://www.mof.gov.sg/singaporebudget/revenue-and-expenditure',\n",
       "   'content': 'SG60 Vouchers - 2.02 : CDC Vouchers: 0.78: 1.06 : Other Transfers 3: 2.29: 0.69 : BASIC SURPLUS / DEFICIT: 0.64 (4.79) ... Other Funds for Revised FY2024 consist of GST Voucher Fund, Financial Sector Development Fund, Edusave Endowment Fund, Progressive Wage Credit Scheme Fund, Majulah Package Fund, Bus Service Enhancement Fund, Skills',\n",
       "   'score': 0.6546525,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 1.73}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Budget Speech Budget Resources Budget 2025 Highlights About Budget Budget 2025 Highlights CDC Vouchers [New]  $500 SG60 ActiveSG Credit Top-Up [New]   $100 SG60 Vouchers [New] $600 or $800 Child LifeSG Credits or Edusave Account / Post-Secondary Education Account Top-up [New] $500 MediSave [GSTV] $150 to $450 Large Family LifeSG Credits [New] SG Culture Pass [New]   $100 Personal Income Tax Rebate for Year of Assessment (YA) 2025 [New]   Up to $200 CDC Vouchers [New]  $300 Support for You and Your Households Singapore Budget 2025 is part of the Ministry of Finance, Singapore. Singapore Budget 2025 Budget Speech Budget Statement Budget Resources Budget 2025 Highlights Support For You And Your Household Support For Vulnerable Families And Persons With Disabilities About Budget Budget Archives\n"
     ]
    }
   ],
   "source": [
    "print(search_res['results'][0]['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import ReActAgent\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "func_calling_llm = LiteLLM(\"ollama_chat/llama3.2:3b\")\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=40000)\n",
    "\n",
    "tools = [search_knowledge_base_tool]\n",
    "agent = ReActAgent(\n",
    "    name=\"Budget 2025 RAG Agent\", \n",
    "    description=\"An agent that tells you information about the budget\",\n",
    "    llm=func_calling_llm,\n",
    "    tools=tools\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.storage.chat_store.postgres import PostgresChatStore\n",
    "\n",
    "chat_store = PostgresChatStore.from_uri(DB_URL)\n",
    "\n",
    "thread_id = 'thread_2'\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(\n",
    "    chat_store=chat_store,\n",
    "    chat_store_key=thread_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = memory.get_all()\n",
    "question = 'as an undergraduate student, what benefits can i get?'\n",
    "handler = agent.run(question, memory=memory, chat_history=chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: search_knowledge_base\n",
      "Action Input: {\"input\": {\"title\": \"Benefits for undergraduate students\", \"type\": \"string\"}}AgentOutput: assistant: Thought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: search_knowledge_base\n",
      "Action Input: {\"input\": {\"title\": \"Benefits for undergraduate students\", \"type\": \"string\"}}\n",
      "Thought: The current language of the user is still English. It seems that I need to provide a valid string as input for the tool.\n",
      "Action: search_knowledge_base\n",
      "Action Input: {'input': 'Benefits for undergraduate students'}AgentOutput: assistant: Thought: The current language of the user is still English. It seems that I need to provide a valid string as input for the tool.\n",
      "Action: search_knowledge_base\n",
      "Action Input: {'input': 'Benefits for undergraduate students'}\n",
      "Thought: The current language of the user is still English. I have obtained relevant information from the tool about benefits for undergraduate students.\n",
      "Answer: Investing in initiatives that support undergraduate students can lead to increased efficiency, improved productivity, and enhanced customer satisfaction, ultimately benefiting both the company's bottom line and reputation, as well as the future of our workforce and economy.AgentOutput: assistant: Thought: The current language of the user is still English. I have obtained relevant information from the tool about benefits for undergraduate students.\n",
      "Answer: Investing in initiatives that support undergraduate students can lead to increased efficiency, improved productivity, and enhanced customer satisfaction, ultimately benefiting both the company's bottom line and reputation, as well as the future of our workforce and economy.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import AgentStream, AgentOutput\n",
    "\n",
    "events = []\n",
    "async for event in handler.stream_events():\n",
    "    events.append(event)\n",
    "    if isinstance(event, AgentStream):\n",
    "        print(event.delta, end=\"\", flush=True)\n",
    "\n",
    "    elif isinstance(event, AgentOutput):\n",
    "       print(f'AgentOutput: {event.response}')  # the current full response\n",
    "    #    print(event.tool_calls)  # the selected tool calls, if any\n",
    "    #    print(event.raw)  # the raw llm api response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe use chat engine will be better than agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrective RAG Workflow \n",
    "- (retrieve_context) Question come in, retrieve context and set in state, maybe perform some postprocessing e.g. reranking\n",
    "- (generate_answer) Answer question from context\n",
    "- (grade_answer) Grade answer\n",
    "- (refine_answer) Refine answer if needed\n",
    "- (provide_answer) Provide answer to user\n",
    "\n",
    "State to maintain:\n",
    "- Question\n",
    "- Context\n",
    "- Answer\n",
    "- Grade\n",
    "- Refined Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    ")\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.core.base.response.schema import RESPONSE_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Start Event\n",
    "class QuestionEvent(StartEvent):\n",
    "    question: Annotated[str, \"Single question\"]\n",
    "    similarity_top_k: Annotated[int, \"Number of nodes to retrieve\"] = 20\n",
    "\n",
    "\n",
    "class NodesRetrievedEvent(Event):\n",
    "    retrieved_nodes: Annotated[list[NodeWithScore], \"Retrieved nodes\"]\n",
    "\n",
    "class NodesRerankedEvent(Event):\n",
    "    reranked_nodes: Annotated[list[NodeWithScore], \"Reranked nodes\"]\n",
    "\n",
    "# class AnswerGeneratedFromContextEvent(Event):\n",
    "class AnswerGeneratedFromContextEvent(StopEvent):\n",
    "    question: Annotated[str, \"Question\"]\n",
    "    context: Annotated[str, \"Context\"]\n",
    "    answer: Annotated[str, \"Answer generated from context\"]\n",
    "\n",
    "class GraderOutput(BaseModel):\n",
    "    is_grounded: bool = Field(description=\"Whether the answer is grounded in the context\")\n",
    "    confidence: float = Field(\n",
    "        gt=0.0, lt=1.0,\n",
    "        description=\"Confidence value between 0.00 and 1.00 of how grounded the answer is obtained from the context.\",\n",
    "    )\n",
    "    confidence_explanation: str = Field(..., description=\"Explanation for the confidence score\")\n",
    "\n",
    "class AnswerGradedEvent(Event):\n",
    "    grader_output: Annotated[GraderOutput, 'Output object from grading the answer with respect to the context']\n",
    "    question: Annotated[str, \"Question\"]\n",
    "    context: Annotated[str, \"Context\"]\n",
    "    answer: Annotated[str, \"Answer\"]\n",
    "    \n",
    "# Stop Event\n",
    "class AnswerGeneratedEvent(StopEvent):\n",
    "    answer: Annotated[str, \"Answer to user's question. If the answer is grounded in the context, then the answer will be the generated answer from context. Otherwise, the answer will be a fallback answer.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "DEFAULT_RELEVANCY_GROUNDING_PROMPT_TEMPLATE = PromptTemplate(\n",
    "    template=\"\"\"As a grader, your task is to evaluate the grounding of a generated answer in the context provided with respect to the user's question.\n",
    "\n",
    "    <question-start>:\n",
    "    \\\"\\\"\\\"\n",
    "    {question_str}\n",
    "    \\\"\\\"\\\"\n",
    "    <question-end>\n",
    "\n",
    "    <context-start>:\n",
    "    \\\"\\\"\\\"\n",
    "    {context_str}\n",
    "    \\\"\\\"\\\"\n",
    "    <context-end>\n",
    "\n",
    "    <answer-start>:\n",
    "    \\\"\\\"\\\"\n",
    "    {answer_str}\n",
    "    \\\"\\\"\\\"\n",
    "    <answer-end>\n",
    "\n",
    "    Evaluation Criteria:\n",
    "    - Consider whether the answer answers the question.\n",
    "    - Consider whether the answer can be inferred from the context.\n",
    "\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "DEFAULT_ANSWER_GENERATION_PROMPT_TEMPLATE = PromptTemplate(\n",
    "    template=\"\"\"Your task is to answer the user's question about Singapore government budget statement based on the context provided. Be detailed and objective in your answer.\n",
    "\n",
    "    Context: \\\"\\\"\\\"\n",
    "    {context_str}\n",
    "    \\\"\\\"\\\"\n",
    "\n",
    "    User Question: \\\"\\\"\\\"\n",
    "    {query_str}\n",
    "    \\\"\\\"\\\"\n",
    "\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'JinaRerank' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[118]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Define the prompt\u001b[39;00m\n\u001b[32m     12\u001b[39m guard_structured_prompt = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[33mQuery string here.\u001b[39m\n\u001b[32m     14\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m \u001b[33m$\u001b[39m\u001b[38;5;132;01m{gr.json_suffix_prompt_v2_wo_none}\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mQAWorkflow\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mWorkflow\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# llm = OpenAI(model=\"gpt-4o-mini\")  # llm to generate answer\u001b[39;49;00m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# llm = Ollama(model=\"llama3.2:3b\", request_timeout=60.0)\u001b[39;49;00m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mLiteLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mollama_chat/llama3.2:3b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvsi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mVectorStoreIndex\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mVectorStoreIndex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_vector_store\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvector_store\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvector_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding_model\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[118]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mQAWorkflow\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     25\u001b[39m llm = LiteLLM(\u001b[33m\"\u001b[39m\u001b[33mollama_chat/llama3.2:3b\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m vsi: VectorStoreIndex = VectorStoreIndex.from_vector_store(\n\u001b[32m     27\u001b[39m     vector_store=vector_store,\n\u001b[32m     28\u001b[39m     embed_model=embedding_model\n\u001b[32m     29\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m postprocessor = \u001b[43mJinaRerank\u001b[49m(\n\u001b[32m     31\u001b[39m     top_n=\u001b[32m20\u001b[39m, model=\u001b[33m\"\u001b[39m\u001b[33mjina-reranker-v1-base-en\u001b[39m\u001b[33m\"\u001b[39m, api_key=JINA_API_KEY\n\u001b[32m     32\u001b[39m )\n\u001b[32m     34\u001b[39m grader_output_guard = gd.Guard.from_pydantic(output_class=GraderOutput, prompt=guard_structured_prompt)\n\u001b[32m     36\u001b[39m grader_output_parser = GuardrailsOutputParser(grader_output_guard)\n",
      "\u001b[31mNameError\u001b[39m: name 'JinaRerank' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.workflow import Context\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.core.response_synthesizers.type import ResponseMode\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from guardrails import Guard\n",
    "import guardrails as gd\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.llms import ChatResponse\n",
    "from llama_index.llms.litellm import LiteLLM\n",
    "\n",
    "# Define the prompt\n",
    "guard_structured_prompt = \"\"\"\n",
    "Query string here.\n",
    "\n",
    "${gr.xml_prefix_prompt}\n",
    "\n",
    "${output_schema}\n",
    "\n",
    "${gr.json_suffix_prompt_v2_wo_none}\n",
    "\"\"\"\n",
    "\n",
    "class QAWorkflow(Workflow):\n",
    "    llm = LiteLLM(\"ollama_chat/llama3.2:3b\")\n",
    "    vsi: VectorStoreIndex = VectorStoreIndex.from_vector_store(\n",
    "        vector_store=vector_store,\n",
    "        embed_model=embedding_model\n",
    "    )\n",
    "    postprocessor = JinaRerank(\n",
    "        top_n=20, model=\"jina-reranker-v1-base-en\", api_key=JINA_API_KEY\n",
    "    )\n",
    "    \n",
    "    grader_output_guard = gd.Guard.from_pydantic(output_class=GraderOutput, prompt=guard_structured_prompt)\n",
    "\n",
    "    grader_output_parser = GuardrailsOutputParser(grader_output_guard)\n",
    "    grader_llm = Ollama(model=\"llama3.2:3b\", request_timeout=60.0, output_parser=grader_output_parser)\n",
    "\n",
    "    @step\n",
    "    async def retrieve_nodes(self, ctx: Context, ev: QuestionEvent) -> NodesRetrievedEvent:\n",
    "        question = ev.question\n",
    "        # set question in global state\n",
    "        await ctx.set(\"question\", question)\n",
    "\n",
    "        retriever = self.vsi.as_retriever(similarity_top_k=ev.similarity_top_k)\n",
    "        qe = vsi.as_query_engine()\n",
    "        nodes = await retriever.aretrieve(question)\n",
    "        \n",
    "        return NodesRetrievedEvent(retrieved_nodes=nodes)\n",
    "\n",
    "    @step\n",
    "    async def rerank_nodes(self, ctx: Context, ev: NodesRetrievedEvent) -> NodesRerankedEvent:\n",
    "        nodes = ev.retrieved_nodes\n",
    "        question = await ctx.get(\"question\")\n",
    "\n",
    "        reranked_nodes = postprocessor.postprocess_nodes(nodes, query_str=question)\n",
    "        return NodesRerankedEvent(reranked_nodes=reranked_nodes)\n",
    "\n",
    "    @step\n",
    "    async def generate_answer_from_context(self, ctx: Context, ev: NodesRerankedEvent) -> AnswerGeneratedFromContextEvent:\n",
    "        nodes = ev.reranked_nodes\n",
    "        question = await ctx.get(\"question\")\n",
    "        print(f'in generate_answer_from_context, question: {question}')\n",
    "\n",
    "        # set context in global state for reference subsequently\n",
    "        context_str = \"\\n\\n\".join([node.get_content(MetadataMode.LLM) for node in nodes])\n",
    "        # await ctx.set(\"context_str\", context_str)\n",
    "\n",
    "        response_synthesizer = get_response_synthesizer(llm=self.llm, response_mode=ResponseMode.COMPACT)\n",
    "        \n",
    "        response = await response_synthesizer.asynthesize(query=question, nodes=nodes)\n",
    "        print(f'in generate_answer_from_context, response: {response}')\n",
    "        \n",
    "        return AnswerGeneratedFromContextEvent(answer=response.response, question=question, context=context_str)\n",
    "\n",
    "    # @step\n",
    "    # async def grade_answer(self, ctx: Context, ev: AnswerGeneratedFromContextEvent) -> AnswerGradedEvent:\n",
    "    #     question, answer, context = ev.question, ev.answer, ev.context\n",
    "    #     print(f'in grade_answer, question: {question}')\n",
    "    #     print(f'in grade_answer, answer: {answer}')\n",
    "\n",
    "    #     grading_template = DEFAULT_RELEVANCY_GROUNDING_PROMPT_TEMPLATE.format(question_str=question, context_str=context, answer_str=answer)\n",
    "\n",
    "    #     grading_template_with_guard = self.grader_output_parser.format(grading_template)\n",
    "\n",
    "    #     grader_output_response: ChatResponse = await self.grader_llm.achat([ChatMessage(role='user', content=grading_template_with_guard)])\n",
    "\n",
    "    #     grader_output_response_str = grader_output_response.message.content\n",
    "    #     grader_output = GraderOutput.model_validate(grader_output_response_str)\n",
    "\n",
    "    #     return AnswerGradedEvent(grader_output=grader_output, question=question, context=context, answer=answer)\n",
    "    \n",
    "    # @step\n",
    "    # async def generate_answer(self, ctx: Context, ev: AnswerGradedEvent) -> AnswerGeneratedEvent:\n",
    "    #     # check if the answer is grounded in the context\n",
    "    #     answer = 'Sorry, I don\\'t have enough information to answer that question.'\n",
    "    #     if ev.grader_output.is_grounded:\n",
    "    #         answer = ev.answer\n",
    "        \n",
    "    #     return AnswerGeneratedEvent(answer=answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step retrieve_nodes\n",
      "Step retrieve_nodes produced event NodesRetrievedEvent\n",
      "Running step rerank_nodes\n",
      "Step rerank_nodes produced event NodesRerankedEvent\n",
      "Running step generate_answer_from_context\n",
      "in generate_answer_from_context, question: What do i gain if i am an undergraduate student?\n",
      "in generate_answer_from_context, response: Based on the new context of changing circumstances, I'll rewrite the answer:\n",
      "\n",
      "As a Singaporean undergraduate student, navigating the current economic landscape can be challenging. However, with the government's emphasis on technology and innovation, enterprise ecosystem, and infrastructure investments, there are opportunities to enhance your skills and knowledge.\n",
      "\n",
      "You can develop in-demand expertise through training programs and workshops that drive efficiency and innovation in various sectors, such as those related to technology and sustainability. Additionally, connecting with like-minded individuals and entrepreneurs who share your vision for creating positive impact can help you harness market forces and strengthen your capabilities.\n",
      "\n",
      "Accessing resources and support to turn your ideas into reality is also crucial. Staying open to people and ideas from diverse backgrounds and industries can further improve standards of living and contribute to the country's development.\n",
      "\n",
      "If you're unsure about how this applies, it remains unchanged:\n",
      "Step generate_answer_from_context produced event AnswerGeneratedFromContextEvent\n",
      "Running step grade_answer\n",
      "in grade_answer, question: What do i gain if i am an undergraduate student?\n",
      "in grade_answer, answer: Based on the new context of changing circumstances, I'll rewrite the answer:\n",
      "\n",
      "As a Singaporean undergraduate student, navigating the current economic landscape can be challenging. However, with the government's emphasis on technology and innovation, enterprise ecosystem, and infrastructure investments, there are opportunities to enhance your skills and knowledge.\n",
      "\n",
      "You can develop in-demand expertise through training programs and workshops that drive efficiency and innovation in various sectors, such as those related to technology and sustainability. Additionally, connecting with like-minded individuals and entrepreneurs who share your vision for creating positive impact can help you harness market forces and strengthen your capabilities.\n",
      "\n",
      "Accessing resources and support to turn your ideas into reality is also crucial. Staying open to people and ideas from diverse backgrounds and industries can further improve standards of living and contribute to the country's development.\n",
      "\n",
      "If you're unsure about how this applies, it remains unchanged:\n"
     ]
    },
    {
     "ename": "WorkflowRuntimeError",
     "evalue": "Error in step 'grade_answer': 1 validation error for GraderOutput\n  Input should be a valid dictionary or instance of GraderOutput [type=model_type, input_value='```json\\n{\\n  \"is_ground...in the context\"\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/budget2025-rag/.venv/lib/python3.12/site-packages/llama_index/core/workflow/context.py:583\u001b[39m, in \u001b[36mContext._step_worker\u001b[39m\u001b[34m(self, name, step, config, stepwise, verbose, checkpoint_callback, run_id, service_manager, dispatcher)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m583\u001b[39m     new_ev = \u001b[38;5;28;01mawait\u001b[39;00m instrumented_step(**kwargs)\n\u001b[32m    584\u001b[39m     kwargs.clear()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/budget2025-rag/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:368\u001b[39m, in \u001b[36mDispatcher.span.<locals>.async_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[134]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36mQAWorkflow.grade_answer\u001b[39m\u001b[34m(self, ctx, ev)\u001b[39m\n\u001b[32m     88\u001b[39m grader_output_response_str = grader_output_response.message.content\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m grader_output = \u001b[43mGraderOutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrader_output_response_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m AnswerGradedEvent(grader_output=grader_output, question=question, context=context, answer=answer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/budget2025-rag/.venv/lib/python3.12/site-packages/pydantic/main.py:703\u001b[39m, in \u001b[36mBaseModel.model_validate\u001b[39m\u001b[34m(cls, obj, strict, from_attributes, context, by_alias, by_name)\u001b[39m\n\u001b[32m    698\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[32m    699\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    700\u001b[39m         code=\u001b[33m'\u001b[39m\u001b[33mvalidate-by-alias-and-name-false\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    701\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_name\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for GraderOutput\n  Input should be a valid dictionary or instance of GraderOutput [type=model_type, input_value='```json\\n{\\n  \"is_ground...in the context\"\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mWorkflowRuntimeError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[135]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m w = QAWorkflow(timeout=\u001b[32m120\u001b[39m,verbose=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m w.run(start_event=QuestionEvent(question=\u001b[33m\"\u001b[39m\u001b[33mWhat do i gain if i am an undergraduate student?\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/budget2025-rag/.venv/lib/python3.12/site-packages/llama_index/core/workflow/workflow.py:394\u001b[39m, in \u001b[36mWorkflow.run.<locals>._run_workflow\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_raised:\n\u001b[32m    391\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    392\u001b[39m     ctx.write_event_to_stream(StopEvent())\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_raised\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m we_done:\n\u001b[32m    397\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    398\u001b[39m     ctx.write_event_to_stream(StopEvent())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/budget2025-rag/.venv/lib/python3.12/site-packages/llama_index/core/workflow/context.py:592\u001b[39m, in \u001b[36mContext._step_worker\u001b[39m\u001b[34m(self, name, step, config, stepwise, verbose, checkpoint_callback, run_id, service_manager, dispatcher)\u001b[39m\n\u001b[32m    590\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    591\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.retry_policy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m592\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m WorkflowRuntimeError(\n\u001b[32m    593\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError in step \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    594\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    596\u001b[39m     delay = config.retry_policy.next(\n\u001b[32m    597\u001b[39m         retry_start_at + time.time(), attempts, e\n\u001b[32m    598\u001b[39m     )\n\u001b[32m    599\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m delay \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    600\u001b[39m         \u001b[38;5;66;03m# We're done retrying\u001b[39;00m\n",
      "\u001b[31mWorkflowRuntimeError\u001b[39m: Error in step 'grade_answer': 1 validation error for GraderOutput\n  Input should be a valid dictionary or instance of GraderOutput [type=model_type, input_value='```json\\n{\\n  \"is_ground...in the context\"\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type"
     ]
    }
   ],
   "source": [
    "w = QAWorkflow(timeout=120,verbose=True)\n",
    "result = await w.run(start_event=QuestionEvent(question=\"What do i gain if i am an undergraduate student?\"))\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_model_name = 'ollama_chat/llama3.2:3b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guardrails import Guard\n",
    "\n",
    "\n",
    "guard = Guard()\n",
    "\n",
    "result = guard(\n",
    "    messages=[{\"role\":\"user\", \"content\":\"How many moons does Jupiter have?\"}],\n",
    "    model=ollama_model_name,\n",
    ")\n",
    "\n",
    "print(f\"{result.validated_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraderOutput(BaseModel):\n",
    "    is_grounded: bool = Field(description=\"Whether the answer is grounded in the context\")\n",
    "    confidence: float = Field(\n",
    "        gt=0.0, lt=1.0,\n",
    "        description=\"Confidence value between 0-1 of how grounded the answer is obtained from the context.\",\n",
    "    )\n",
    "    confidence_explanation: str = Field(..., description=\"Explanation for the confidence score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "guard = Guard.for_pydantic(GraderOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'What benefits do i get if i am an undergraduate student?'\n",
    "context = 'Singapore government provides various benefits to undergraduate students. For example, they can apply for the Singaporean government scholarship to study in Singapore. They can also apply for the Singaporean government loan to study in Singapore. They can also apply for the Singaporean government grant to study in Singapore.'\n",
    "# answer = 'The capital of France is Paris.'\n",
    "answer = 'Students can apply for the Singaporean government scholarship, take a loan or apply for a government grant to study in Singapore.'\n",
    "\n",
    "\n",
    "prompt = DEFAULT_RELEVANCY_GROUNDING_PROMPT_TEMPLATE.format(question_str=question, context_str=context, answer_str=answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\n",
    "  \"role\": \"system\",\n",
    "  \"content\": \"You are a helpful assistant.\"\n",
    "}, {\n",
    "  \"role\": \"user\",\n",
    "  \"content\": prompt\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mightymagnus/projects/budget2025-rag/.venv/lib/python3.12/site-packages/guardrails/validator_service/__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prompt+=\"\"\"\n",
    "\n",
    "${gr.complete_json_suffix_v3}\n",
    "\"\"\"\n",
    "response = guard(\n",
    "    model=ollama_model_name,\n",
    "    messages=messages,\n",
    "    # prompt_params={\"chat_history\": chat_history},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_grounded': True,\n",
       " 'confidence': 0.7,\n",
       " 'confidence_explanation': 'Partially. The context provides information about various government benefits available to undergraduate students, but it does not explicitly mention a scholarship, loan, or grant.'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.validated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'is_grounded': True, 'confidence': 0.7, 'confidence_explanation': 'Partially. The context provides information about various government benefits available to undergraduate students, but it does not explicitly mention a scholarship, loan, or grant.'}\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_str = str(response.validated_output)\n",
    "response_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [] # an open ai compatible list of tools\n",
    "\n",
    "response = guard(\n",
    "    model=ollama_model_name,\n",
    "    messages=messages,\n",
    "    prompt_params={\"chat_history\": chat_history},\n",
    "    tools=guard.json_function_calling_tool(tools),\n",
    "    tool_choice=\"required\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
