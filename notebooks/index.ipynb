{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mistralai import Mistral\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.envrc')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "import llama_index.core\n",
    "\n",
    "def launch_phoenix():\n",
    "    px.launch_app()\n",
    "    llama_index.core.set_global_handler(\"arize_phoenix\")\n",
    "\n",
    "def close_phoenix():\n",
    "    px.close_app()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingestion Setup for PDF Files\n",
    "- Parse PDF to text\n",
    "- Select embedding model\n",
    "- Connection to vector store\n",
    "    - Create table if not exists\n",
    "- Create index pipeline\n",
    "- Run the pipeline on the parsed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = 'fy2025_budget_statement.pdf'\n",
    "# file_name = 'budget-debate-round-up-speech.pdf'\n",
    "# file_name = 'fy2025_budget_booklet_english.pdf'\n",
    "# file_name = 'fy2025_budget_booklet_chinese.pdf'\n",
    "# file_name = 'fy2025_budget_booklet_malay.pdf'\n",
    "file_name = 'fy2025_budget_booklet_tamil.pdf'\n",
    "file_path = f'../data/{file_name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert PDF to text\n",
    "\n",
    "Use [Mistral OCR API](https://docs.mistral.ai/capabilities/document/) for PDF parsing because of support for\n",
    "- Parse PDF into markdown\n",
    "- Allow for images in pdf (base64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "assert MISTRAL_API_KEY is not None\n",
    "client = Mistral(api_key=MISTRAL_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signed_url: url='https://mistralaifilesapiprodswe.blob.core.windows.net/fine-tune/ca9f74b9-2aeb-457f-a3ac-a81ac3401e24/ce5a4f6f5f06405790b1a51df7ad7543.pdf?se=2025-04-16T08%3A35%3A18Z&sp=r&sv=2025-05-05&sr=b&sig=eLu6UF9z/SZOrnql2z9rZ3Pn9%2BSC1VX1IOKgX/bF63M%3D'\n"
     ]
    }
   ],
   "source": [
    "uploaded_pdf = client.files.upload(\n",
    "    file={\n",
    "        \"file_name\": file_name,\n",
    "        \"content\": open(file_path, \"rb\"),\n",
    "    },\n",
    "    purpose=\"ocr\"\n",
    ")\n",
    "\n",
    "signed_url = client.files.get_signed_url(file_id=uploaded_pdf.id)\n",
    "print(f'signed_url: {signed_url}')\n",
    "\n",
    "# Send to Mistral OCR API\n",
    "ocr_response = client.ocr.process(\n",
    "            model=\"mistral-ocr-latest\",\n",
    "            document={\n",
    "                \"type\": \"document_url\",\n",
    "                \"document_url\": signed_url.url,\n",
    "            },\n",
    "            include_image_base64=False,\n",
    "        )\n",
    "        \n",
    "markdown = '\\n\\n'.join([page.markdown for page in ocr_response.pages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the markdown\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(markdown))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract from image (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract from image (not needed)\n",
    "# ocr_response = client.ocr.process(\n",
    "#     model=\"mistral-ocr-latest\",\n",
    "#     document={\n",
    "#         \"type\": \"image_url\",\n",
    "#         \"image_url\": \"https://www.mof.gov.sg/docs/librariesprovider3/budget2025/images/resources/fy2025_budget_disbursement_calendar_english.png\"\n",
    "#     }\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import re\n",
    "\n",
    "model_name = \"intfloat/multilingual-e5-large\"\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=model_name)\n",
    "\n",
    "embedding_model_dimensions = 1024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_HOST = os.getenv('DB_HOST')\n",
    "assert DB_HOST is not None\n",
    "DB_PORT = os.getenv('DB_PORT')\n",
    "assert DB_PORT is not None\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "assert DB_USER is not None\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "assert DB_PASSWORD is not None\n",
    "DB_NAME = os.getenv('DB_NAME')\n",
    "assert DB_NAME is not None\n",
    "\n",
    "DB_URL = f'postgresql+asyncpg://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create database and table (Initial setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'budget_2025-intfloat-multilingual-e5-large'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "table_prefix = 'budget_2025-'\n",
    "# regex to only have words, numbers, and dashes\n",
    "# replace / with -\n",
    "model_name_clean = re.sub(r'[^a-zA-Z0-9\\-]', '-', model_name)\n",
    "table_name = f'{table_prefix}{model_name_clean}'\n",
    "table_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized for budget_2025-intfloat-multilingual-e5-large\n"
     ]
    }
   ],
   "source": [
    "# Change this to True if creating the table for the first time\n",
    "perform_setup = False\n",
    "vector_store = PGVectorStore.from_params(\n",
    "            host=DB_HOST,\n",
    "            port=DB_PORT,\n",
    "            database=DB_NAME,\n",
    "            user=DB_USER,\n",
    "            password=DB_PASSWORD,\n",
    "            table_name=table_name,\n",
    "            perform_setup=perform_setup,\n",
    "            embed_dim=embedding_model_dimensions,\n",
    "        )\n",
    "\n",
    "if perform_setup:\n",
    "    vector_store._initialize()\n",
    "    print(f'Vector store initialized for {table_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vsi = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create index pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "\n",
    "semantic_splitter = SemanticSplitterNodeParser(embed_model=embed_model)\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        MarkdownNodeParser(),\n",
    "        semantic_splitter,\n",
    "        embed_model\n",
    "    ],\n",
    "    vector_store=vector_store\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 created for fy2025_budget_booklet_tamil.pdf\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.schema import Document\n",
    "\n",
    "metadata = {\n",
    "    'source_document': file_name\n",
    "}\n",
    "\n",
    "document = Document(text=markdown, metadata=metadata)\n",
    "document.excluded_embed_metadata_keys = metadata.keys()\n",
    "\n",
    "nodes = await pipeline.arun(documents=[document])\n",
    "print(f'{len(nodes)} created for {file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_top_k = 30\n",
    "retriever = vsi.as_retriever(similarity_top_k=similarity_top_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbedding(model_name='intfloat/multilingual-e5-large', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7f10ba05ddc0>, num_workers=None, max_length=512, normalize=True, query_instruction=None, text_instruction=None, cache_folder=None, show_progress_bar=False)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vsi._embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are some housing subsidies provided in the budget?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8069594377017614\n",
      "The measures we have taken in recent years, and are taking in this Budget, will help to mitigate the impact of rising costs.\n",
      "31. But in the longer term, the best way to adjust to higher prices is to grow the economy and increase productivity, so that all Singaporeans can enjoy higher real incomes and better standards of living. And let me turn to our strategies next in these areas.\n"
     ]
    }
   ],
   "source": [
    "# question = \"How much is the government topping up to cultural matching fund?\"\n",
    "\n",
    "nodes = retriever.retrieve(question)\n",
    "print(nodes[0].score)\n",
    "print(nodes[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent with Tools\n",
    "Tools:\n",
    "- Search from knowledge base (QueryEngineTool)\n",
    "- Search from web (FunctionTool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Query Engine Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LiteLLM(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7f10ba05ddc0>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x7f118598a340>, completion_to_prompt=<function default_completion_to_prompt at 0x7f1185751260>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model='ollama_chat/llama3.2:3b', temperature=0.1, max_tokens=None, additional_kwargs={}, max_retries=10)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_synthesizer._llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "query_engine = vsi.as_query_engine(\n",
    "    llm=response_synthesizer._llm,\n",
    "    similarity_top_k=similarity_top_k,\n",
    "    node_postprocessors=[similarity_postprocessor, reranker],\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "search_knowledge_base_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine,\n",
    "    name='search_knowledge_base',\n",
    "    description=\"Search information from the knowledge base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web Search Tool\n",
    "Web search just in case the answer is not retrieved from the knowledge base? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAVILY_API_KEY = os.getenv('TAVILY_API_KEY')\n",
    "assert TAVILY_API_KEY is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from tavily import AsyncTavilyClient\n",
    "\n",
    "async def search_web(query: str) -> str:\n",
    "    \"\"\"Useful for using the web to answer questions.\"\"\"\n",
    "    client = AsyncTavilyClient(api_key=\"tvly-...\")\n",
    "    return str(await client.search(query))\n",
    "\n",
    "\n",
    "tool = FunctionTool.from_defaults(\n",
    "    search_web,\n",
    "    name='search_web',\n",
    "    description=\"Useful for using the web to answer questions\"\n",
    "    # async_fn=aget_weather,  # optional!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import AsyncTavilyClient\n",
    "\n",
    "client = AsyncTavilyClient(api_key=TAVILY_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how much CDC vouchers can I get?\"\n",
    "\n",
    "include_domains = ['https://www.mof.gov.sg/singaporebudget']\n",
    "\n",
    "search_res = await client.search(query=query, include_domains=include_domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'how much CDC vouchers can I get?',\n",
       " 'follow_up_questions': None,\n",
       " 'answer': None,\n",
       " 'images': [],\n",
       " 'results': [{'title': 'Budget | Support For Singaporeans',\n",
       "   'url': 'https://www.mof.gov.sg/singaporebudget/budget-2025-highlights/support-for-singaporeans',\n",
       "   'content': 'Budget Speech Budget Resources Budget 2025 Highlights About Budget Budget 2025 Highlights CDC Vouchers [New]  $500 SG60 ActiveSG Credit Top-Up [New]   $100 SG60 Vouchers [New] $600 or $800 Child LifeSG Credits or Edusave Account / Post-Secondary Education Account Top-up [New] $500 MediSave [GSTV] $150 to $450 Large Family LifeSG Credits [New] SG Culture Pass [New]   $100 Personal Income Tax Rebate for Year of Assessment (YA) 2025 [New]   Up to $200 CDC Vouchers [New]  $300 Support for You and Your Households Singapore Budget 2025 is part of the Ministry of Finance, Singapore. Singapore Budget 2025 Budget Speech Budget Statement Budget Resources Budget 2025 Highlights Support For You And Your Household Support For Vulnerable Families And Persons With Disabilities About Budget Budget Archives',\n",
       "   'score': 0.78846955,\n",
       "   'raw_content': None},\n",
       "  {'title': 'Revenue and Expenditure - Budget',\n",
       "   'url': 'https://www.mof.gov.sg/singaporebudget/revenue-and-expenditure',\n",
       "   'content': 'SG60 Vouchers - 2.02 : CDC Vouchers: 0.78: 1.06 : Other Transfers 3: 2.29: 0.69 : BASIC SURPLUS / DEFICIT: 0.64 (4.79) ... Other Funds for Revised FY2024 consist of GST Voucher Fund, Financial Sector Development Fund, Edusave Endowment Fund, Progressive Wage Credit Scheme Fund, Majulah Package Fund, Bus Service Enhancement Fund, Skills',\n",
       "   'score': 0.6546525,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 1.73}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Budget Speech Budget Resources Budget 2025 Highlights About Budget Budget 2025 Highlights CDC Vouchers [New]  $500 SG60 ActiveSG Credit Top-Up [New]   $100 SG60 Vouchers [New] $600 or $800 Child LifeSG Credits or Edusave Account / Post-Secondary Education Account Top-up [New] $500 MediSave [GSTV] $150 to $450 Large Family LifeSG Credits [New] SG Culture Pass [New]   $100 Personal Income Tax Rebate for Year of Assessment (YA) 2025 [New]   Up to $200 CDC Vouchers [New]  $300 Support for You and Your Households Singapore Budget 2025 is part of the Ministry of Finance, Singapore. Singapore Budget 2025 Budget Speech Budget Statement Budget Resources Budget 2025 Highlights Support For You And Your Household Support For Vulnerable Families And Persons With Disabilities About Budget Budget Archives\n"
     ]
    }
   ],
   "source": [
    "print(search_res['results'][0]['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import ReActAgent\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "func_calling_llm = LiteLLM(\"ollama_chat/llama3.2:3b\")\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=40000)\n",
    "\n",
    "tools = [search_knowledge_base_tool]\n",
    "agent = ReActAgent(\n",
    "    name=\"Budget 2025 RAG Agent\", \n",
    "    description=\"An agent that tells you information about the budget\",\n",
    "    llm=func_calling_llm,\n",
    "    tools=tools\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.storage.chat_store.postgres import PostgresChatStore\n",
    "\n",
    "chat_store = PostgresChatStore.from_uri(DB_URL)\n",
    "\n",
    "thread_id = 'thread_2'\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(\n",
    "    chat_store=chat_store,\n",
    "    chat_store_key=thread_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = memory.get_all()\n",
    "question = 'as an undergraduate student, what benefits can i get?'\n",
    "handler = agent.run(question, memory=memory, chat_history=chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: search_knowledge_base\n",
      "Action Input: {\"input\": {\"title\": \"Benefits for undergraduate students\", \"type\": \"string\"}}AgentOutput: assistant: Thought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: search_knowledge_base\n",
      "Action Input: {\"input\": {\"title\": \"Benefits for undergraduate students\", \"type\": \"string\"}}\n",
      "Thought: The current language of the user is still English. It seems that I need to provide a valid string as input for the tool.\n",
      "Action: search_knowledge_base\n",
      "Action Input: {'input': 'Benefits for undergraduate students'}AgentOutput: assistant: Thought: The current language of the user is still English. It seems that I need to provide a valid string as input for the tool.\n",
      "Action: search_knowledge_base\n",
      "Action Input: {'input': 'Benefits for undergraduate students'}\n",
      "Thought: The current language of the user is still English. I have obtained relevant information from the tool about benefits for undergraduate students.\n",
      "Answer: Investing in initiatives that support undergraduate students can lead to increased efficiency, improved productivity, and enhanced customer satisfaction, ultimately benefiting both the company's bottom line and reputation, as well as the future of our workforce and economy.AgentOutput: assistant: Thought: The current language of the user is still English. I have obtained relevant information from the tool about benefits for undergraduate students.\n",
      "Answer: Investing in initiatives that support undergraduate students can lead to increased efficiency, improved productivity, and enhanced customer satisfaction, ultimately benefiting both the company's bottom line and reputation, as well as the future of our workforce and economy.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import AgentStream, AgentOutput\n",
    "\n",
    "events = []\n",
    "async for event in handler.stream_events():\n",
    "    events.append(event)\n",
    "    if isinstance(event, AgentStream):\n",
    "        print(event.delta, end=\"\", flush=True)\n",
    "\n",
    "    elif isinstance(event, AgentOutput):\n",
    "       print(f'AgentOutput: {event.response}')  # the current full response\n",
    "    #    print(event.tool_calls)  # the selected tool calls, if any\n",
    "    #    print(event.raw)  # the raw llm api response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe use chat engine will be better than agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corrective RAG Workflow \n",
    "- (retrieve_context) Question come in, retrieve context and set in state, maybe perform some postprocessing e.g. reranking\n",
    "- (generate_answer) Answer question from context\n",
    "- (grade_answer) Grade answer\n",
    "- (refine_answer) Refine answer if needed\n",
    "- (provide_answer) Provide answer to user\n",
    "\n",
    "State to maintain:\n",
    "- Question\n",
    "- Context\n",
    "- Answer\n",
    "- Grade\n",
    "- Refined Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    ")\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.core.base.response.schema import RESPONSE_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Start Event\n",
    "class QuestionEvent(StartEvent):\n",
    "    question: Annotated[str, \"Single question\"]\n",
    "    similarity_top_k: Annotated[int, \"Number of nodes to retrieve\"] = 20\n",
    "\n",
    "\n",
    "class NodesRetrievedEvent(Event):\n",
    "    retrieved_nodes: Annotated[list[NodeWithScore], \"Retrieved nodes\"]\n",
    "\n",
    "class NodesRerankedEvent(Event):\n",
    "    reranked_nodes: Annotated[list[NodeWithScore], \"Reranked nodes\"]\n",
    "\n",
    "# class AnswerGeneratedFromContextEvent(Event):\n",
    "class AnswerGeneratedFromContextEvent(StopEvent):\n",
    "    question: Annotated[str, \"Question\"]\n",
    "    context: Annotated[str, \"Context\"]\n",
    "    answer: Annotated[str, \"Answer generated from context\"]\n",
    "\n",
    "class GraderOutput(BaseModel):\n",
    "    is_grounded: bool = Field(description=\"Whether the answer is grounded in the context\")\n",
    "    confidence: float = Field(\n",
    "        gt=0.0, lt=1.0,\n",
    "        description=\"Confidence value between 0.00 and 1.00 of how grounded the answer is obtained from the context.\",\n",
    "    )\n",
    "    confidence_explanation: str = Field(..., description=\"Explanation for the confidence score\")\n",
    "\n",
    "class AnswerGradedEvent(Event):\n",
    "    grader_output: Annotated[GraderOutput, 'Output object from grading the answer with respect to the context']\n",
    "    question: Annotated[str, \"Question\"]\n",
    "    context: Annotated[str, \"Context\"]\n",
    "    answer: Annotated[str, \"Answer\"]\n",
    "    \n",
    "# Stop Event\n",
    "class AnswerGeneratedEvent(StopEvent):\n",
    "    answer: Annotated[str, \"Answer to user's question. If the answer is grounded in the context, then the answer will be the generated answer from context. Otherwise, the answer will be a fallback answer.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "DEFAULT_RELEVANCY_GROUNDING_PROMPT_TEMPLATE = PromptTemplate(\n",
    "    template=\"\"\"As a grader, your task is to evaluate the grounding of a generated answer in the context provided with respect to the user's question.\n",
    "\n",
    "    <question-start>:\n",
    "    \\\"\\\"\\\"\n",
    "    {question_str}\n",
    "    \\\"\\\"\\\"\n",
    "    <question-end>\n",
    "\n",
    "    <context-start>:\n",
    "    \\\"\\\"\\\"\n",
    "    {context_str}\n",
    "    \\\"\\\"\\\"\n",
    "    <context-end>\n",
    "\n",
    "    <answer-start>:\n",
    "    \\\"\\\"\\\"\n",
    "    {answer_str}\n",
    "    \\\"\\\"\\\"\n",
    "    <answer-end>\n",
    "\n",
    "    Evaluation Criteria:\n",
    "    - Consider whether the answer answers the question.\n",
    "    - Consider whether the answer can be inferred from the context.\n",
    "\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.2:3b\", request_timeout=60.0)  # 8.2s to answer question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor.jinaai_rerank import JinaRerank\n",
    "\n",
    "JINA_API_KEY = os.getenv('JINA_API_KEY')\n",
    "assert JINA_API_KEY is not None\n",
    "\n",
    "postprocessor = JinaRerank(\n",
    "    top_n=20, model=\"jina-reranker-v1-base-en\", api_key=JINA_API_KEY\n",
    ")\n",
    "\n",
    "# Testing\n",
    "# reranked_nodes = postprocessor.postprocess_nodes(nodes, query_str=question)\n",
    "# len(reranked_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "DEFAULT_ANSWER_GENERATION_PROMPT_TEMPLATE = PromptTemplate(\n",
    "    template=\"\"\"Your task is to answer the user's question about Singapore government budget statement based on the context provided. Be detailed and objective in your answer.\n",
    "\n",
    "    Context: \\\"\\\"\\\"\n",
    "    {context_str}\n",
    "    \\\"\\\"\\\"\n",
    "\n",
    "    User Question: \\\"\\\"\\\"\n",
    "    {query_str}\n",
    "    \\\"\\\"\\\"\n",
    "\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'JinaRerank' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[118]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Define the prompt\u001b[39;00m\n\u001b[32m     12\u001b[39m guard_structured_prompt = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[33mQuery string here.\u001b[39m\n\u001b[32m     14\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m \u001b[33m$\u001b[39m\u001b[38;5;132;01m{gr.json_suffix_prompt_v2_wo_none}\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mQAWorkflow\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mWorkflow\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# llm = OpenAI(model=\"gpt-4o-mini\")  # llm to generate answer\u001b[39;49;00m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# llm = Ollama(model=\"llama3.2:3b\", request_timeout=60.0)\u001b[39;49;00m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mLiteLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mollama_chat/llama3.2:3b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvsi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mVectorStoreIndex\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mVectorStoreIndex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_vector_store\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvector_store\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvector_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding_model\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[118]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mQAWorkflow\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     25\u001b[39m llm = LiteLLM(\u001b[33m\"\u001b[39m\u001b[33mollama_chat/llama3.2:3b\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m vsi: VectorStoreIndex = VectorStoreIndex.from_vector_store(\n\u001b[32m     27\u001b[39m     vector_store=vector_store,\n\u001b[32m     28\u001b[39m     embed_model=embedding_model\n\u001b[32m     29\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m postprocessor = \u001b[43mJinaRerank\u001b[49m(\n\u001b[32m     31\u001b[39m     top_n=\u001b[32m20\u001b[39m, model=\u001b[33m\"\u001b[39m\u001b[33mjina-reranker-v1-base-en\u001b[39m\u001b[33m\"\u001b[39m, api_key=JINA_API_KEY\n\u001b[32m     32\u001b[39m )\n\u001b[32m     34\u001b[39m grader_output_guard = gd.Guard.from_pydantic(output_class=GraderOutput, prompt=guard_structured_prompt)\n\u001b[32m     36\u001b[39m grader_output_parser = GuardrailsOutputParser(grader_output_guard)\n",
      "\u001b[31mNameError\u001b[39m: name 'JinaRerank' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.workflow import Context\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.core.response_synthesizers.type import ResponseMode\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from guardrails import Guard\n",
    "import guardrails as gd\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.llms import ChatResponse\n",
    "from llama_index.llms.litellm import LiteLLM\n",
    "\n",
    "# Define the prompt\n",
    "guard_structured_prompt = \"\"\"\n",
    "Query string here.\n",
    "\n",
    "${gr.xml_prefix_prompt}\n",
    "\n",
    "${output_schema}\n",
    "\n",
    "${gr.json_suffix_prompt_v2_wo_none}\n",
    "\"\"\"\n",
    "\n",
    "class QAWorkflow(Workflow):\n",
    "    # llm = OpenAI(model=\"gpt-4o-mini\")  # llm to generate answer\n",
    "    # llm = Ollama(model=\"llama3.2:3b\", request_timeout=60.0)\n",
    "    llm = LiteLLM(\"ollama_chat/llama3.2:3b\")\n",
    "    vsi: VectorStoreIndex = VectorStoreIndex.from_vector_store(\n",
    "        vector_store=vector_store,\n",
    "        embed_model=embedding_model\n",
    "    )\n",
    "    postprocessor = JinaRerank(\n",
    "        top_n=20, model=\"jina-reranker-v1-base-en\", api_key=JINA_API_KEY\n",
    "    )\n",
    "    \n",
    "    grader_output_guard = gd.Guard.from_pydantic(output_class=GraderOutput, prompt=guard_structured_prompt)\n",
    "\n",
    "    grader_output_parser = GuardrailsOutputParser(grader_output_guard)\n",
    "    grader_llm = Ollama(model=\"llama3.2:3b\", request_timeout=60.0, output_parser=grader_output_parser)\n",
    "\n",
    "    @step\n",
    "    async def retrieve_nodes(self, ctx: Context, ev: QuestionEvent) -> NodesRetrievedEvent:\n",
    "        question = ev.question\n",
    "        # set question in global state\n",
    "        await ctx.set(\"question\", question)\n",
    "\n",
    "        retriever = self.vsi.as_retriever(similarity_top_k=ev.similarity_top_k)\n",
    "        qe = vsi.as_query_engine()\n",
    "        nodes = await retriever.aretrieve(question)\n",
    "        \n",
    "        return NodesRetrievedEvent(retrieved_nodes=nodes)\n",
    "\n",
    "    @step\n",
    "    async def rerank_nodes(self, ctx: Context, ev: NodesRetrievedEvent) -> NodesRerankedEvent:\n",
    "        nodes = ev.retrieved_nodes\n",
    "        question = await ctx.get(\"question\")\n",
    "\n",
    "        reranked_nodes = postprocessor.postprocess_nodes(nodes, query_str=question)\n",
    "        return NodesRerankedEvent(reranked_nodes=reranked_nodes)\n",
    "\n",
    "    @step\n",
    "    async def generate_answer_from_context(self, ctx: Context, ev: NodesRerankedEvent) -> AnswerGeneratedFromContextEvent:\n",
    "        nodes = ev.reranked_nodes\n",
    "        question = await ctx.get(\"question\")\n",
    "        print(f'in generate_answer_from_context, question: {question}')\n",
    "\n",
    "        # set context in global state for reference subsequently\n",
    "        context_str = \"\\n\\n\".join([node.get_content(MetadataMode.LLM) for node in nodes])\n",
    "        # await ctx.set(\"context_str\", context_str)\n",
    "\n",
    "        response_synthesizer = get_response_synthesizer(llm=self.llm, response_mode=ResponseMode.COMPACT)\n",
    "        \n",
    "        response = await response_synthesizer.asynthesize(query=question, nodes=nodes)\n",
    "        print(f'in generate_answer_from_context, response: {response}')\n",
    "        \n",
    "        return AnswerGeneratedFromContextEvent(answer=response.response, question=question, context=context_str)\n",
    "\n",
    "    # @step\n",
    "    # async def grade_answer(self, ctx: Context, ev: AnswerGeneratedFromContextEvent) -> AnswerGradedEvent:\n",
    "    #     question, answer, context = ev.question, ev.answer, ev.context\n",
    "    #     print(f'in grade_answer, question: {question}')\n",
    "    #     print(f'in grade_answer, answer: {answer}')\n",
    "\n",
    "    #     grading_template = DEFAULT_RELEVANCY_GROUNDING_PROMPT_TEMPLATE.format(question_str=question, context_str=context, answer_str=answer)\n",
    "\n",
    "    #     grading_template_with_guard = self.grader_output_parser.format(grading_template)\n",
    "\n",
    "    #     grader_output_response: ChatResponse = await self.grader_llm.achat([ChatMessage(role='user', content=grading_template_with_guard)])\n",
    "\n",
    "    #     grader_output_response_str = grader_output_response.message.content\n",
    "    #     grader_output = GraderOutput.model_validate(grader_output_response_str)\n",
    "\n",
    "    #     return AnswerGradedEvent(grader_output=grader_output, question=question, context=context, answer=answer)\n",
    "    \n",
    "    # @step\n",
    "    # async def generate_answer(self, ctx: Context, ev: AnswerGradedEvent) -> AnswerGeneratedEvent:\n",
    "    #     # check if the answer is grounded in the context\n",
    "    #     answer = 'Sorry, I don\\'t have enough information to answer that question.'\n",
    "    #     if ev.grader_output.is_grounded:\n",
    "    #         answer = ev.answer\n",
    "        \n",
    "    #     return AnswerGeneratedEvent(answer=answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LiteLLM(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7f6774a90fb0>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x7f6775afa700>, completion_to_prompt=<function default_completion_to_prompt at 0x7f67758c9620>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model='ollama_chat/llama3.2:3b', temperature=0.1, max_tokens=None, additional_kwargs={}, max_retries=10)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = vsi.as_chat_engine(llm=llm, no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response='As an undergraduate student, you may benefit from various initiatives such as lifelong learning programs, skills development opportunities, and support services that cater to your needs. These initiatives can help you develop valuable skills, enhance your employability, and achieve your career goals. Additionally, you may be eligible for financial assistance or scholarships to support your education and personal development.', sources=[ToolOutput(content='There is no mention of undergraduate students in the provided context. The context primarily discusses initiatives related to lifelong learning, skills development, and support for workers, particularly mid-career Singaporeans and lower-wage workers, as well as measures to help households cope with utilities expenses.', tool_name='query_engine_tool', raw_input={'input': 'What are the benefits for an undergraduate student'}, raw_output=Response(response='There is no mention of undergraduate students in the provided context. The context primarily discusses initiatives related to lifelong learning, skills development, and support for workers, particularly mid-career Singaporeans and lower-wage workers, as well as measures to help households cope with utilities expenses.', source_nodes=[NodeWithScore(node=TextNode(id_='71c93807-7135-4a79-9ad9-dc5cb6b97142', embedding=None, metadata={'header_path': '/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e0b6873d-732f-4695-8557-7ee49b77cd4a', node_type='4', metadata={}, hash='81234a12461f7de6957ecd8f1a86edaf4aa23539f9464ffa68f80c09cbccbe00'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='32d57f36-5315-483b-bdbd-60b08a841f04', node_type='1', metadata={'header_path': '/'}, hash='0d6d82f826a32394cb1ae9659437513973caa9b30571fb4f515eef6dbab663a3'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='86c96071-b4fd-4c9c-ba7c-1a24c2f4702a', node_type='1', metadata={'header_path': '/'}, hash='9f8224b55a47d310ca171e6c39a98b790ddf197f7eba658b1d141a04d445b31d')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"# Encouraging Lifelong Learning \\n\\n70. At last year's Budget, I introduced the SkillsFuture Level-Up Programme, to support mid-career Singaporeans in getting a significant skills reboot.\\na. Under the programme, all Singaporeans aged 40 years and above will get $\\\\$ 4,000$ in SkillsFuture Credit.\\nb. From next month, individuals can apply for a training allowance of up to $\\\\$ 3,000$ per month for selected full-time courses. They will have 24 months' worth of allowance, which is up to $\\\\$ 72,000$ per worker.\\n71. Some Singaporeans prefer to continue working, while upskilling on a part-time basis.\\na. Take for example, Ms Zarina Binti Abdul Rahman. She started out as an intern in Randstad Sourceright, a global recruitment agency, and is now an office administrator.\\n\\nb. She wants to upgrade her skills in business operations, but is unable to pursue full-time studies due to her work and family commitments. So she is pursuing a part-time Diploma in Business Practice at Nanyang Polytechnic.\\n72. While workers like Zarina continue to earn their salaries during part-time training, they also incur training-related expenses.\\na. We will therefore extend the training allowance to workers undergoing part-time training.\\nb. They can enjoy a fixed allowance of $\\\\$ 300$ per month to defray their learning expenses.\\nc. And this part-time training allowance will be implemented early next year.\\nd. The Minister for Education will share more at the Committee of Supply.\\n\\n73. We will also give a further boost to our lower-wage workers, who will benefit from early upskilling.\\na. Today, our lower-wage workers can tap on the Workfare Skills Support Scheme from the age of 30.\\nb. The scheme provides employers with absentee payroll support when they sponsor workers for training. It also provides workers with a training allowance when they go for self-sponsored training.\\n74. Currently, the Workfare Skills Support Scheme is designed primarily to support short courses that are completed over a few days. But lower-wage workers stand to benefit more from longerform courses that provide more substantial reskilling and upskilling.\\n75. We will therefore introduce an enhanced tier of support under the Workfare Skills Support for lower-wage workers. This enhanced support will be modelled after the SkillsFuture Level-Up Programme, and will benefit lower-wage workers when they turn 30. So this will start earlier at age 30. The Minister for Manpower will share more details at the Committee of Supply (COS).\\n\\n76. Our Institutes of Higher Learning are key partners in supporting lifelong learning. They equip our workers with relevant skills for a changing economy, and provide a dedicated setting for deep learning.\\na. We will support the Singapore University of Social Sciences (or SUSS) in developing a new city campus. This will enable SUSS to champion lifelong learning and deliver programmes with a strong social emphasis, at an accessible location in the city for learners of all ages.\\nb. We will also extend the Singapore Universities Trust by 10 years to 31 March 2042 to support the fundraising efforts of our newer Autonomous Universities, namely SUSS and the Singapore Institute of Technology. The Trust provides matching grants for donations, to support our Autonomous Universities in building up their endowment funds.\\n77. Sir, the Government is fully committed to supporting the lifelong employability of our workers. We cannot stop the waves of technological innovations. Nor can we save every job. But we can and we will invest in every worker and every Singaporean. We will equip everyone with the skills to adapt, compete and succeed in a changing economy.\", mimetype='text/plain', start_char_idx=30712, end_char_idx=34379, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.271217866467369), NodeWithScore(node=TextNode(id_='819372e4-d23b-42a2-93cb-c76e09edd759', embedding=None, metadata={'header_path': '/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e0b6873d-732f-4695-8557-7ee49b77cd4a', node_type='4', metadata={}, hash='81234a12461f7de6957ecd8f1a86edaf4aa23539f9464ffa68f80c09cbccbe00'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='7116dcd9-14db-414c-8b90-d0fcce8da744', node_type='1', metadata={'header_path': '/'}, hash='4a8adaf5eeede4689ce525974f637c7193b5d3f7358bac607937405341afa783'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='9238fcf5-66ad-4e34-a219-a0e7543a8dfe', node_type='1', metadata={'header_path': '/b. Second, I will provide additional U-Save rebates to help households cope with their utilities expenses. Eligible HDB households will receive up to $\\\\$ 760$ this financial year. This is double the amount of regular U-Save rebates. It will cover about six months of utilities expenses for those living in 1and 2-room flats, and three months of utilities expenses for those living in 3- and 4-room flats. /'}, hash='8de51262b12ae6b6bd9b102f1c1e25c70845f00bb747691574649c8bffb45930')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='# b. Second, I will provide additional U-Save rebates to help households cope with their utilities expenses. Eligible HDB households will receive up to $\\\\$ 760$ this financial year. This is double the amount of regular U-Save rebates. It will cover about six months of utilities expenses for those living in 1and 2-room flats, and three months of utilities expenses for those living in 3- and 4-room flats.', mimetype='text/plain', start_char_idx=7118, end_char_idx=7524, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.25360564110228934)], metadata={'71c93807-7135-4a79-9ad9-dc5cb6b97142': {'header_path': '/'}, '819372e4-d23b-42a2-93cb-c76e09edd759': {'header_path': '/'}}), is_error=False)], source_nodes=[NodeWithScore(node=TextNode(id_='71c93807-7135-4a79-9ad9-dc5cb6b97142', embedding=None, metadata={'header_path': '/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e0b6873d-732f-4695-8557-7ee49b77cd4a', node_type='4', metadata={}, hash='81234a12461f7de6957ecd8f1a86edaf4aa23539f9464ffa68f80c09cbccbe00'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='32d57f36-5315-483b-bdbd-60b08a841f04', node_type='1', metadata={'header_path': '/'}, hash='0d6d82f826a32394cb1ae9659437513973caa9b30571fb4f515eef6dbab663a3'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='86c96071-b4fd-4c9c-ba7c-1a24c2f4702a', node_type='1', metadata={'header_path': '/'}, hash='9f8224b55a47d310ca171e6c39a98b790ddf197f7eba658b1d141a04d445b31d')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"# Encouraging Lifelong Learning \\n\\n70. At last year's Budget, I introduced the SkillsFuture Level-Up Programme, to support mid-career Singaporeans in getting a significant skills reboot.\\na. Under the programme, all Singaporeans aged 40 years and above will get $\\\\$ 4,000$ in SkillsFuture Credit.\\nb. From next month, individuals can apply for a training allowance of up to $\\\\$ 3,000$ per month for selected full-time courses. They will have 24 months' worth of allowance, which is up to $\\\\$ 72,000$ per worker.\\n71. Some Singaporeans prefer to continue working, while upskilling on a part-time basis.\\na. Take for example, Ms Zarina Binti Abdul Rahman. She started out as an intern in Randstad Sourceright, a global recruitment agency, and is now an office administrator.\\n\\nb. She wants to upgrade her skills in business operations, but is unable to pursue full-time studies due to her work and family commitments. So she is pursuing a part-time Diploma in Business Practice at Nanyang Polytechnic.\\n72. While workers like Zarina continue to earn their salaries during part-time training, they also incur training-related expenses.\\na. We will therefore extend the training allowance to workers undergoing part-time training.\\nb. They can enjoy a fixed allowance of $\\\\$ 300$ per month to defray their learning expenses.\\nc. And this part-time training allowance will be implemented early next year.\\nd. The Minister for Education will share more at the Committee of Supply.\\n\\n73. We will also give a further boost to our lower-wage workers, who will benefit from early upskilling.\\na. Today, our lower-wage workers can tap on the Workfare Skills Support Scheme from the age of 30.\\nb. The scheme provides employers with absentee payroll support when they sponsor workers for training. It also provides workers with a training allowance when they go for self-sponsored training.\\n74. Currently, the Workfare Skills Support Scheme is designed primarily to support short courses that are completed over a few days. But lower-wage workers stand to benefit more from longerform courses that provide more substantial reskilling and upskilling.\\n75. We will therefore introduce an enhanced tier of support under the Workfare Skills Support for lower-wage workers. This enhanced support will be modelled after the SkillsFuture Level-Up Programme, and will benefit lower-wage workers when they turn 30. So this will start earlier at age 30. The Minister for Manpower will share more details at the Committee of Supply (COS).\\n\\n76. Our Institutes of Higher Learning are key partners in supporting lifelong learning. They equip our workers with relevant skills for a changing economy, and provide a dedicated setting for deep learning.\\na. We will support the Singapore University of Social Sciences (or SUSS) in developing a new city campus. This will enable SUSS to champion lifelong learning and deliver programmes with a strong social emphasis, at an accessible location in the city for learners of all ages.\\nb. We will also extend the Singapore Universities Trust by 10 years to 31 March 2042 to support the fundraising efforts of our newer Autonomous Universities, namely SUSS and the Singapore Institute of Technology. The Trust provides matching grants for donations, to support our Autonomous Universities in building up their endowment funds.\\n77. Sir, the Government is fully committed to supporting the lifelong employability of our workers. We cannot stop the waves of technological innovations. Nor can we save every job. But we can and we will invest in every worker and every Singaporean. We will equip everyone with the skills to adapt, compete and succeed in a changing economy.\", mimetype='text/plain', start_char_idx=30712, end_char_idx=34379, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.271217866467369), NodeWithScore(node=TextNode(id_='819372e4-d23b-42a2-93cb-c76e09edd759', embedding=None, metadata={'header_path': '/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e0b6873d-732f-4695-8557-7ee49b77cd4a', node_type='4', metadata={}, hash='81234a12461f7de6957ecd8f1a86edaf4aa23539f9464ffa68f80c09cbccbe00'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='7116dcd9-14db-414c-8b90-d0fcce8da744', node_type='1', metadata={'header_path': '/'}, hash='4a8adaf5eeede4689ce525974f637c7193b5d3f7358bac607937405341afa783'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='9238fcf5-66ad-4e34-a219-a0e7543a8dfe', node_type='1', metadata={'header_path': '/b. Second, I will provide additional U-Save rebates to help households cope with their utilities expenses. Eligible HDB households will receive up to $\\\\$ 760$ this financial year. This is double the amount of regular U-Save rebates. It will cover about six months of utilities expenses for those living in 1and 2-room flats, and three months of utilities expenses for those living in 3- and 4-room flats. /'}, hash='8de51262b12ae6b6bd9b102f1c1e25c70845f00bb747691574649c8bffb45930')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='# b. Second, I will provide additional U-Save rebates to help households cope with their utilities expenses. Eligible HDB households will receive up to $\\\\$ 760$ this financial year. This is double the amount of regular U-Save rebates. It will cover about six months of utilities expenses for those living in 1and 2-room flats, and three months of utilities expenses for those living in 3- and 4-room flats.', mimetype='text/plain', start_char_idx=7118, end_char_idx=7524, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.25360564110228934)], is_dummy_stream=False, metadata=None)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_engine.chat(\"What do i benefit as an undergraduate student?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What do i benefit as an undergraduate student?\"\n",
    "nodes = vsi.as_retriever(similarity_top_k=20).retrieve(question)\n",
    "\n",
    "reranked_nodes = postprocessor.postprocess_nodes(nodes, query_str=question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streaming litellm\n",
    "llm = LiteLLM(\"ollama_chat/llama3.2:3b\")\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(llm=llm, response_mode=ResponseMode.COMPACT)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await response_synthesizer.asynthesize(query=question, nodes=nodes, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step retrieve_nodes\n",
      "Step retrieve_nodes produced event NodesRetrievedEvent\n",
      "Running step rerank_nodes\n",
      "Step rerank_nodes produced event NodesRerankedEvent\n",
      "Running step generate_answer_from_context\n",
      "in generate_answer_from_context, question: What do i gain if i am an undergraduate student?\n",
      "in generate_answer_from_context, response: Based on the new context of changing circumstances, I'll rewrite the answer:\n",
      "\n",
      "As a Singaporean undergraduate student, navigating the current economic landscape can be challenging. However, with the government's emphasis on technology and innovation, enterprise ecosystem, and infrastructure investments, there are opportunities to enhance your skills and knowledge.\n",
      "\n",
      "You can develop in-demand expertise through training programs and workshops that drive efficiency and innovation in various sectors, such as those related to technology and sustainability. Additionally, connecting with like-minded individuals and entrepreneurs who share your vision for creating positive impact can help you harness market forces and strengthen your capabilities.\n",
      "\n",
      "Accessing resources and support to turn your ideas into reality is also crucial. Staying open to people and ideas from diverse backgrounds and industries can further improve standards of living and contribute to the country's development.\n",
      "\n",
      "If you're unsure about how this applies, it remains unchanged:\n",
      "Step generate_answer_from_context produced event AnswerGeneratedFromContextEvent\n",
      "Running step grade_answer\n",
      "in grade_answer, question: What do i gain if i am an undergraduate student?\n",
      "in grade_answer, answer: Based on the new context of changing circumstances, I'll rewrite the answer:\n",
      "\n",
      "As a Singaporean undergraduate student, navigating the current economic landscape can be challenging. However, with the government's emphasis on technology and innovation, enterprise ecosystem, and infrastructure investments, there are opportunities to enhance your skills and knowledge.\n",
      "\n",
      "You can develop in-demand expertise through training programs and workshops that drive efficiency and innovation in various sectors, such as those related to technology and sustainability. Additionally, connecting with like-minded individuals and entrepreneurs who share your vision for creating positive impact can help you harness market forces and strengthen your capabilities.\n",
      "\n",
      "Accessing resources and support to turn your ideas into reality is also crucial. Staying open to people and ideas from diverse backgrounds and industries can further improve standards of living and contribute to the country's development.\n",
      "\n",
      "If you're unsure about how this applies, it remains unchanged:\n"
     ]
    },
    {
     "ename": "WorkflowRuntimeError",
     "evalue": "Error in step 'grade_answer': 1 validation error for GraderOutput\n  Input should be a valid dictionary or instance of GraderOutput [type=model_type, input_value='```json\\n{\\n  \"is_ground...in the context\"\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/budget2025-rag/.venv/lib/python3.12/site-packages/llama_index/core/workflow/context.py:583\u001b[39m, in \u001b[36mContext._step_worker\u001b[39m\u001b[34m(self, name, step, config, stepwise, verbose, checkpoint_callback, run_id, service_manager, dispatcher)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m583\u001b[39m     new_ev = \u001b[38;5;28;01mawait\u001b[39;00m instrumented_step(**kwargs)\n\u001b[32m    584\u001b[39m     kwargs.clear()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/budget2025-rag/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:368\u001b[39m, in \u001b[36mDispatcher.span.<locals>.async_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[134]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36mQAWorkflow.grade_answer\u001b[39m\u001b[34m(self, ctx, ev)\u001b[39m\n\u001b[32m     88\u001b[39m grader_output_response_str = grader_output_response.message.content\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m grader_output = \u001b[43mGraderOutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrader_output_response_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m AnswerGradedEvent(grader_output=grader_output, question=question, context=context, answer=answer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/budget2025-rag/.venv/lib/python3.12/site-packages/pydantic/main.py:703\u001b[39m, in \u001b[36mBaseModel.model_validate\u001b[39m\u001b[34m(cls, obj, strict, from_attributes, context, by_alias, by_name)\u001b[39m\n\u001b[32m    698\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[32m    699\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    700\u001b[39m         code=\u001b[33m'\u001b[39m\u001b[33mvalidate-by-alias-and-name-false\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    701\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_name\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for GraderOutput\n  Input should be a valid dictionary or instance of GraderOutput [type=model_type, input_value='```json\\n{\\n  \"is_ground...in the context\"\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mWorkflowRuntimeError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[135]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m w = QAWorkflow(timeout=\u001b[32m120\u001b[39m,verbose=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m w.run(start_event=QuestionEvent(question=\u001b[33m\"\u001b[39m\u001b[33mWhat do i gain if i am an undergraduate student?\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/budget2025-rag/.venv/lib/python3.12/site-packages/llama_index/core/workflow/workflow.py:394\u001b[39m, in \u001b[36mWorkflow.run.<locals>._run_workflow\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_raised:\n\u001b[32m    391\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    392\u001b[39m     ctx.write_event_to_stream(StopEvent())\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_raised\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m we_done:\n\u001b[32m    397\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    398\u001b[39m     ctx.write_event_to_stream(StopEvent())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/budget2025-rag/.venv/lib/python3.12/site-packages/llama_index/core/workflow/context.py:592\u001b[39m, in \u001b[36mContext._step_worker\u001b[39m\u001b[34m(self, name, step, config, stepwise, verbose, checkpoint_callback, run_id, service_manager, dispatcher)\u001b[39m\n\u001b[32m    590\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    591\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.retry_policy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m592\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m WorkflowRuntimeError(\n\u001b[32m    593\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError in step \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    594\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    596\u001b[39m     delay = config.retry_policy.next(\n\u001b[32m    597\u001b[39m         retry_start_at + time.time(), attempts, e\n\u001b[32m    598\u001b[39m     )\n\u001b[32m    599\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m delay \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    600\u001b[39m         \u001b[38;5;66;03m# We're done retrying\u001b[39;00m\n",
      "\u001b[31mWorkflowRuntimeError\u001b[39m: Error in step 'grade_answer': 1 validation error for GraderOutput\n  Input should be a valid dictionary or instance of GraderOutput [type=model_type, input_value='```json\\n{\\n  \"is_ground...in the context\"\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type"
     ]
    }
   ],
   "source": [
    "w = QAWorkflow(timeout=120,verbose=True)\n",
    "result = await w.run(start_event=QuestionEvent(question=\"What do i gain if i am an undergraduate student?\"))\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total budget for the year is not explicitly stated in the provided information. However, it mentions expected surpluses for FY2024 and FY2025, which are projected to be \\$6.4 billion and \\$6.8 billion, respectively. For a specific total budget figure, additional details would be required.\n"
     ]
    }
   ],
   "source": [
    "ans: RESPONSE_TYPE = result.answer\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the capital of France?\"\n",
    "context = \"The capital of France is Paris.\"\n",
    "answer = \"The capital of France is Paris.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sllm = llm.as_structured_llm(output_cls=GraderOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "grade_prompt = DEFAULT_RELEVANCY_GROUNDING_PROMPT_TEMPLATE.format(context_str=context, answer_str=answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = sllm.chat([ChatMessage(role=\"user\", content=prompt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"is_grounded\":true,\"confidence\":0.8,\"confidence_explanation\":\"The answer matches the exact wording of the context, which indicates a strong logical connection and use of relevant keywords.\"}'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LiteLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.litellm import LiteLLM\n",
    "from llama_index.core.llms import ChatMessage\n",
    "# openai call\n",
    "llm = LiteLLM(\"ollama_chat/llama3.2:3b\")\n",
    "\n",
    "messages = [ChatMessage(role=\"user\", content='hello tell me a joke about singapore')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response = llm.chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Here's one:\n",
      "\n",
      "Why did the chili crab from Singapore go to therapy?\n",
      "\n",
      "Because it was feeling a little \"steamed\"!\n",
      "\n",
      "(Sorry, I know it's a bit of a seafood pun, but I hope it made you crack a smile!)\n"
     ]
    }
   ],
   "source": [
    "print(chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = await llm.astream_chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Here's one:\n",
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "(wait for it...)\n",
      "\n",
      "An impasta!\n",
      "\n",
      "Hope that made you smile! Do you want to hear another one?\n"
     ]
    }
   ],
   "source": [
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why did the chili crab from Singapore go to therapy?\n",
      "\n",
      "Because it was feeling a little \"steamed\"!\n",
      "\n",
      "(Sorry, I couldn't resist the seafood pun!)"
     ]
    }
   ],
   "source": [
    "async for r in resp:\n",
    "    print(r.delta, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_model_name = 'ollama_chat/llama3.2:3b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mightymagnus/projects/budget2025-rag/.venv/lib/python3.12/site-packages/guardrails/validator_service/__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupiter has a total of 92 confirmed moons. However, it's worth noting that this number may not be definitive, as there are many smaller, irregular moons that have not been officially confirmed or named.\n",
      "\n",
      "The four largest and most well-known moons of Jupiter are:\n",
      "\n",
      "1. Io\n",
      "2. Europa\n",
      "3. Ganymede\n",
      "4. Callisto\n",
      "\n",
      "These four moons are known as the Galilean moons, as they were discovered by Galileo Galilei in 1610.\n"
     ]
    }
   ],
   "source": [
    "from guardrails import Guard\n",
    "\n",
    "\n",
    "guard = Guard()\n",
    "\n",
    "result = guard(\n",
    "    messages=[{\"role\":\"user\", \"content\":\"How many moons does Jupiter have?\"}],\n",
    "    model=ollama_model_name,\n",
    ")\n",
    "\n",
    "print(f\"{result.validated_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraderOutput(BaseModel):\n",
    "    is_grounded: bool = Field(description=\"Whether the answer is grounded in the context\")\n",
    "    confidence: float = Field(\n",
    "        gt=0.0, lt=1.0,\n",
    "        description=\"Confidence value between 0-1 of how grounded the answer is obtained from the context.\",\n",
    "    )\n",
    "    confidence_explanation: str = Field(..., description=\"Explanation for the confidence score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "guard = Guard.for_pydantic(GraderOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'What benefits do i get if i am an undergraduate student?'\n",
    "context = 'Singapore government provides various benefits to undergraduate students. For example, they can apply for the Singaporean government scholarship to study in Singapore. They can also apply for the Singaporean government loan to study in Singapore. They can also apply for the Singaporean government grant to study in Singapore.'\n",
    "# answer = 'The capital of France is Paris.'\n",
    "answer = 'Students can apply for the Singaporean government scholarship, take a loan or apply for a government grant to study in Singapore.'\n",
    "\n",
    "\n",
    "prompt = DEFAULT_RELEVANCY_GROUNDING_PROMPT_TEMPLATE.format(question_str=question, context_str=context, answer_str=answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\n",
    "  \"role\": \"system\",\n",
    "  \"content\": \"You are a helpful assistant.\"\n",
    "}, {\n",
    "  \"role\": \"user\",\n",
    "  \"content\": prompt\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mightymagnus/projects/budget2025-rag/.venv/lib/python3.12/site-packages/guardrails/validator_service/__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prompt+=\"\"\"\n",
    "\n",
    "${gr.complete_json_suffix_v3}\n",
    "\"\"\"\n",
    "response = guard(\n",
    "    model=ollama_model_name,\n",
    "    messages=messages,\n",
    "    # prompt_params={\"chat_history\": chat_history},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_grounded': True,\n",
       " 'confidence': 0.7,\n",
       " 'confidence_explanation': 'Partially. The context provides information about various government benefits available to undergraduate students, but it does not explicitly mention a scholarship, loan, or grant.'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.validated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'is_grounded': True, 'confidence': 0.7, 'confidence_explanation': 'Partially. The context provides information about various government benefits available to undergraduate students, but it does not explicitly mention a scholarship, loan, or grant.'}\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_str = str(response.validated_output)\n",
    "response_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[91]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.9-linux-x86_64-gnu/lib/python3.12/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.9-linux-x86_64-gnu/lib/python3.12/json/decoder.py:338\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n\u001b[32m    334\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    335\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n\u001b[32m    336\u001b[39m \n\u001b[32m    337\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m     end = _w(s, end).end()\n\u001b[32m    340\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.9-linux-x86_64-gnu/lib/python3.12/json/decoder.py:356\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    354\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n\u001b[32m    355\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "json.loads(data_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"deliveries\": [\n",
      "    {\n",
      "      \"date\": \"June 3\",\n",
      "      \"pickup\": {\n",
      "        \"address\": \"797 9th Avenue, Manila\",\n",
      "        \"time\": \"10:00am\"\n",
      "      },\n",
      "      \"dropoff\": {\n",
      "        \"address\": \"Courthouse, 61 Center Street C/O frank james\",\n",
      "        \"time\": \"10:30am\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"June 2\",\n",
      "      \"pickup\": {\n",
      "        \"address\": \"21 3rd Street\",\n",
      "        \"time\": \"11:00am\",\n",
      "        \"item\": \"flowers\",\n",
      "        \"cost\": 14.50\n",
      "      },\n",
      "      \"dropoff\": {\n",
      "        \"address\": \"75th Ave\",\n",
      "        \"time\": \"5:30pm\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"June 3\",\n",
      "      \"pickup\": {\n",
      "        \"address\": \"331 5th Street\",\n",
      "        \"time\": \"11:00am\",\n",
      "        \"item\": \"bagels\",\n",
      "        \"cost\": 34.50\n",
      "      },\n",
      "      \"dropoff\": {\n",
      "        \"address\": \"75th Ave\",\n",
      "        \"time\": \"5:30pm\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response.validated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if isinstance(response.validated_output, str):\n",
    "    \n",
    "else:\n",
    "    Schedule.model_validate(response.validated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Schedule\n  Input should be an object [type=model_type, input_value='```\\n{\\n  \"deliveries\": ...  }\\n    }\\n  ]\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mSchedule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_validate_strings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidated_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# response.validated_output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/budget2025-rag/.venv/lib/python3.12/site-packages/pydantic/main.py:779\u001b[39m, in \u001b[36mBaseModel.model_validate_strings\u001b[39m\u001b[34m(cls, obj, strict, context, by_alias, by_name)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m by_alias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[32m    775\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    776\u001b[39m         code=\u001b[33m'\u001b[39m\u001b[33mvalidate-by-alias-and-name-false\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    777\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m779\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_strings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_name\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for Schedule\n  Input should be an object [type=model_type, input_value='```\\n{\\n  \"deliveries\": ...  }\\n    }\\n  ]\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type"
     ]
    }
   ],
   "source": [
    "Schedule.model_validate_strings(response.validated_output)\n",
    "\n",
    "# response.validated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [] # an open ai compatible list of tools\n",
    "\n",
    "response = guard(\n",
    "    model=ollama_model_name,\n",
    "    messages=messages,\n",
    "    prompt_params={\"chat_history\": chat_history},\n",
    "    tools=guard.json_function_calling_tool(tools),\n",
    "    tool_choice=\"required\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Create a guard object\n",
    "guard = gd.Guard.from_pydantic(output_class=GraderOutput)\n",
    "\n",
    "# Create output parse object\n",
    "output_parser = GuardrailsOutputParser(guard)\n",
    "\n",
    "# attach to an llm object\n",
    "grader_llm = Ollama(model=\"llama3.2:3b\", request_timeout=60.0, output_parser=output_parser)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.completions.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "PromptCallableException",
     "evalue": "The callable `fn` passed to `Guard(fn, ...)` failed with the following error: `1 validation error for LLMResponse\noutput\n  Input should be a valid string [type=string_type, input_value=CompletionResponse(text='...gprobs=None, delta=None), input_type=CompletionResponse]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type`. Make sure that `fn` can be called as a function that takes in a single prompt string and returns a string.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/budget2025-rag/.venv/lib/python3.12/site-packages/guardrails/llm_providers.py:46\u001b[39m, in \u001b[36mPromptCallableBase.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke_llm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/budget2025-rag/.venv/lib/python3.12/site-packages/guardrails/llm_providers.py:517\u001b[39m, in \u001b[36mArbitraryCallable._invoke_llm\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    516\u001b[39m llm_response = cast(\u001b[38;5;28mstr\u001b[39m, llm_response)\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLLMResponse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/budget2025-rag/.venv/lib/python3.12/site-packages/pydantic/main.py:253\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    252\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for LLMResponse\noutput\n  Input should be a valid string [type=string_type, input_value=CompletionResponse(text='...gprobs=None, delta=None), input_type=CompletionResponse]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPromptCallableException\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[86]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m guard_res = \u001b[43mguard\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_api\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrader_llm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomplete\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfmt_qa_tmpl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m guard_res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/budget2025-rag/.venv/lib/python3.12/site-packages/guardrails/guard.py:500\u001b[39m, in \u001b[36mGuard.__call__\u001b[39m\u001b[34m(self, llm_api, prompt_params, num_reasks, prompt, instructions, msg_history, metadata, full_schema_reask, *args, **kwargs)\u001b[39m\n\u001b[32m    485\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_sync(\n\u001b[32m    486\u001b[39m         llm_api,\n\u001b[32m    487\u001b[39m         prompt_params=prompt_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    496\u001b[39m         **kwargs,\n\u001b[32m    497\u001b[39m     )\n\u001b[32m    499\u001b[39m guard_context = contextvars.Context()\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mguard_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[43m__call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm_api\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_reasks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfull_schema_reask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/budget2025-rag/.venv/lib/python3.12/site-packages/guardrails/guard.py:485\u001b[39m, in \u001b[36mGuard.__call__.<locals>.__call\u001b[39m\u001b[34m(self, llm_api, prompt_params, num_reasks, prompt, instructions, msg_history, metadata, full_schema_reask, *args, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_async(\n\u001b[32m    472\u001b[39m         llm_api,\n\u001b[32m    473\u001b[39m         prompt_params=prompt_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    482\u001b[39m         **kwargs,\n\u001b[32m    483\u001b[39m     )\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# Otherwise, call the LLM synchronously\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm_api\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_reasks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_reasks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m=\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_history\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfull_schema_reask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_schema_reask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcall_log\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcall_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/budget2025-rag/.venv/lib/python3.12/site-packages/guardrails/guard.py:575\u001b[39m, in \u001b[36mGuard._call_sync\u001b[39m\u001b[34m(self, llm_api, prompt_params, num_reasks, prompt, instructions, msg_history, metadata, full_schema_reask, call_log, *args, **kwargs)\u001b[39m\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m start_action(action_type=\u001b[33m\"\u001b[39m\u001b[33mguard_call\u001b[39m\u001b[33m\"\u001b[39m, prompt_params=prompt_params):\n\u001b[32m    561\u001b[39m     runner = Runner(\n\u001b[32m    562\u001b[39m         instructions=instructions_obj,\n\u001b[32m    563\u001b[39m         prompt=prompt_obj,\n\u001b[32m   (...)\u001b[39m\u001b[32m    573\u001b[39m         full_schema_reask=full_schema_reask,\n\u001b[32m    574\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m     call = \u001b[43mrunner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall_log\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcall_log\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    576\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ValidationOutcome[OT].from_guard_history(call)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/budget2025-rag/.venv/lib/python3.12/site-packages/guardrails/run.py:229\u001b[39m, in \u001b[36mRunner.__call__\u001b[39m\u001b[34m(self, call_log, prompt_params)\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# Because Pydantic v1 doesn't respect property setters\u001b[39;00m\n\u001b[32m    228\u001b[39m     call_log._exception = e\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m call_log\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/budget2025-rag/.venv/lib/python3.12/site-packages/guardrails/run.py:179\u001b[39m, in \u001b[36mRunner.__call__\u001b[39m\u001b[34m(self, call_log, prompt_params)\u001b[39m\n\u001b[32m    176\u001b[39m index = \u001b[32m0\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_reasks + \u001b[32m1\u001b[39m):\n\u001b[32m    178\u001b[39m     \u001b[38;5;66;03m# Run a single step.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     iteration = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m        \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m=\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmsg_history\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m        \u001b[49m\u001b[43minstructions_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[43minstructions_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmsg_history_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_history_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcall_log\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcall_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m     \u001b[38;5;66;03m# Loop again?\u001b[39;00m\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.do_loop(index, iteration.reasks):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/budget2025-rag/.venv/lib/python3.12/site-packages/guardrails/utils/telemetry_utils.py:216\u001b[39m, in \u001b[36mtrace.<locals>.trace_wrapper.<locals>.to_trace_or_not_to_trace\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    214\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/budget2025-rag/.venv/lib/python3.12/site-packages/guardrails/run.py:341\u001b[39m, in \u001b[36mRunner.step\u001b[39m\u001b[34m(self, index, api, instructions, prompt, msg_history, prompt_params, prompt_schema, instructions_schema, msg_history_schema, output_schema, call_log, output)\u001b[39m\n\u001b[32m    339\u001b[39m     iteration.outputs.error = error_message\n\u001b[32m    340\u001b[39m     iteration.outputs.exception = e\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m iteration\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/budget2025-rag/.venv/lib/python3.12/site-packages/guardrails/run.py:302\u001b[39m, in \u001b[36mRunner.step\u001b[39m\u001b[34m(self, index, api, instructions, prompt, msg_history, prompt_params, prompt_schema, instructions_schema, msg_history_schema, output_schema, call_log, output)\u001b[39m\n\u001b[32m    299\u001b[39m iteration.inputs.msg_history = msg_history\n\u001b[32m    301\u001b[39m \u001b[38;5;66;03m# Call: run the API.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m llm_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m iteration.outputs.llm_response_info = llm_response\n\u001b[32m    307\u001b[39m raw_output = llm_response.output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/budget2025-rag/.venv/lib/python3.12/site-packages/guardrails/utils/telemetry_utils.py:216\u001b[39m, in \u001b[36mtrace.<locals>.trace_wrapper.<locals>.to_trace_or_not_to_trace\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    214\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/budget2025-rag/.venv/lib/python3.12/site-packages/guardrails/run.py:572\u001b[39m, in \u001b[36mRunner.call\u001b[39m\u001b[34m(self, index, instructions, prompt, msg_history, api, output)\u001b[39m\n\u001b[32m    570\u001b[39m     llm_response = api_fn(prompt.source, instructions=instructions.source)\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m prompt:\n\u001b[32m--> \u001b[39m\u001b[32m572\u001b[39m     llm_response = \u001b[43mapi_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m.\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    574\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mmsg_history\u001b[39m\u001b[33m'\u001b[39m\u001b[33m must be provided.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/budget2025-rag/.venv/lib/python3.12/site-packages/guardrails/llm_providers.py:50\u001b[39m, in \u001b[36mPromptCallableBase.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._invoke_llm(\n\u001b[32m     47\u001b[39m         *\u001b[38;5;28mself\u001b[39m.init_args, *args, **\u001b[38;5;28mself\u001b[39m.init_kwargs, **kwargs\n\u001b[32m     48\u001b[39m     )\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PromptCallableException(\n\u001b[32m     51\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe callable `fn` passed to `Guard(fn, ...)` failed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     52\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m with the following error: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     53\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMake sure that `fn` can be called as a function that\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     54\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m takes in a single prompt string \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     55\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mand returns a string.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     56\u001b[39m     )\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, LLMResponse):\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PromptCallableException(\n\u001b[32m     59\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe callable `fn` passed to `Guard(fn, ...)` returned\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     60\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m a non-string value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     63\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mand returns a string.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     64\u001b[39m     )\n",
      "\u001b[31mPromptCallableException\u001b[39m: The callable `fn` passed to `Guard(fn, ...)` failed with the following error: `1 validation error for LLMResponse\noutput\n  Input should be a valid string [type=string_type, input_value=CompletionResponse(text='...gprobs=None, delta=None), input_type=CompletionResponse]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type`. Make sure that `fn` can be called as a function that takes in a single prompt string and returns a string."
     ]
    }
   ],
   "source": [
    "guard_res = guard(llm_api=grader_llm.complete, prompt=fmt_qa_tmpl)\n",
    "guard_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser.parse(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4322/1398668521.py:1: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  res.message.dict()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': <MessageRole.ASSISTANT: 'assistant'>,\n",
       " 'additional_kwargs': {'tool_calls': []},\n",
       " 'blocks': [{'block_type': 'text',\n",
       "   'text': 'To evaluate the grounding of the generated answer in the provided context, let\\'s break down the evaluation criteria:\\n\\n1. **Context Analysis**: The context contains a clear statement about the capital of France. It presents this information in a straightforward manner without any ambiguity or uncertainty.\\n\\n2. **Answer Content**: The answer also states that \"The capital of France is Paris.\" This directly mirrors the content found within the provided context.\\n\\n3. **Grounding Evaluation**:\\n   - **Contextual Relevance**: Both the context and the answer are highly relevant to each other, as they both refer to the same piece of information (the capital of France).\\n   - **Directness**: The answer is not only directly related to the topic but it\\'s also a direct repetition or paraphrasing of the statement found in the context. It does not introduce new information that isn\\'t already present within the original text.\\n   \\nGiven these observations, the generated answer effectively demonstrates grounding by mirroring both the content and structure of the provided context.\\n\\n**Evaluation Conclusion**: The groundness of the answer can be considered excellent, given its direct correspondence with the context\\'s key piece of information. It effectively leverages the existing knowledge presented in the context to provide a coherent response that is also relevant and accurate within that context.'}]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.message.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n",
      "\n",
      "\n",
      "Given below is XML that describes the information to extract from this document and the tags to extract it into.\n",
      "\n",
      "\n",
      "<output>\n",
      "    <bool name=\"is_grounded\" description=\"Whether the answer is grounded in the context\"/>\n",
      "    <float name=\"confidence\" description=\"Confidence value between 0-1 of the correctness of the result.\"/>\n",
      "    <string name=\"confidence_explanation\" description=\"Explanation for the confidence score\"/>\n",
      "</output>\n",
      "\n",
      "\n",
      "\n",
      "ONLY return a valid JSON object (no other text is necessary). The JSON MUST conform to the XML format, including any types and format requests e.g. requests for lists, objects and specific types. Be correct and concise.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.prompts.default_prompts import (\n",
    "    DEFAULT_TEXT_QA_PROMPT_TMPL,\n",
    ")\n",
    "\n",
    "# take a look at the new QA template!\n",
    "fmt_qa_tmpl = output_parser.format(DEFAULT_TEXT_QA_PROMPT_TMPL)\n",
    "print(fmt_qa_tmpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query string here.\n",
      "\n",
      "${gr.xml_prefix_prompt}\n",
      "\n",
      "${output_schema}\n",
      "\n",
      "${gr.json_suffix_prompt_v2_wo_none}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader_llm.chat([ChatMessage(role=\"user\", content=prompt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(DEFAULT_TEXT_QA_PROMPT_TMPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n",
      "\n",
      "\n",
      "Given below is XML that describes the information to extract from this document and the tags to extract it into.\n",
      "\n",
      "\n",
      "<output>\n",
      "    <object name=\"points\" description=\"Bullet points regarding events in the author's life.\">\n",
      "        <string name=\"explanation\"/>\n",
      "        <string name=\"explanation2\"/>\n",
      "        <string name=\"explanation3\"/>\n",
      "    </object>\n",
      "</output>\n",
      "\n",
      "\n",
      "\n",
      "ONLY return a valid JSON object (no other text is necessary). The JSON MUST conform to the XML format, including any types and format requests e.g. requests for lists, objects and specific types. Be correct and concise.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.prompts.default_prompts import (\n",
    "    DEFAULT_TEXT_QA_PROMPT_TMPL,\n",
    ")\n",
    "\n",
    "# take a look at the new QA template!\n",
    "fmt_qa_tmpl = output_parser.format(DEFAULT_TEXT_QA_PROMPT_TMPL)\n",
    "print(fmt_qa_tmpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Guard.__call__() missing 1 required positional argument: 'llm_api'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Set your openai API key here\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# os.environ[\"OPENAI_API_KEY\"] = [YOUR API KEY]\u001b[39;00m\n\u001b[32m      7\u001b[39m guard = Guard()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m res = \u001b[43mguard\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-3.5-turbo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHow do I make a cake?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(res.raw_llm_output)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(res.validated_output)\n",
      "\u001b[31mTypeError\u001b[39m: Guard.__call__() missing 1 required positional argument: 'llm_api'"
     ]
    }
   ],
   "source": [
    "from guardrails import Guard\n",
    "import os\n",
    "\n",
    "# Set your openai API key here\n",
    "# os.environ[\"OPENAI_API_KEY\"] = [YOUR API KEY]\n",
    "\n",
    "guard = Guard()\n",
    "\n",
    "res = guard(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How do I make a cake?\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(res.raw_llm_output)\n",
    "print(res.validated_output)\n",
    "print(res.validation_passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor.jinaai_rerank import JinaRerank\n",
    "\n",
    "JINA_API_KEY = os.getenv('JINA_API_KEY')\n",
    "assert JINA_API_KEY is not None\n",
    "\n",
    "postprocessor = JinaRerank(\n",
    "    top_n=20, model=\"jina-reranker-v1-base-en\", api_key=JINA_API_KEY\n",
    ")\n",
    "\n",
    "# Testing\n",
    "# reranked_nodes = postprocessor.postprocess_nodes(nodes, query_str=question)\n",
    "# len(reranked_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "DEFAULT_ANSWER_GENERATION_PROMPT_TEMPLATE = PromptTemplate(\n",
    "    template=\"\"\"Your task is to answer the user's question about Singapore government budget statement based on the context provided. Be detailed and objective in your answer.\n",
    "\n",
    "    Context: \\\"\\\"\\\"\n",
    "    {context_str}\n",
    "    \\\"\\\"\\\"\n",
    "\n",
    "    User Question: \\\"\\\"\\\"\n",
    "    {query_str}\n",
    "    \\\"\\\"\\\"\n",
    "\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "question = \"What is the capital of France?\"\n",
    "# llm = Ollama(model=\"tinyllama\", request_timeout=60.0)  # slow, took 37.8s to answer question\n",
    "llm = Ollama(model=\"llama3.2:3b\", request_timeout=60.0)  # 8.2s to answer question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResponse(message=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={'tool_calls': []}, blocks=[TextBlock(block_type='text', text='The capital of France is Paris.')]), raw={'model': 'llama3.2:3b', 'created_at': '2025-04-12T16:00:33.990030653Z', 'done': True, 'done_reason': 'stop', 'total_duration': 8223440021, 'load_duration': 7548821024, 'prompt_eval_count': 32, 'prompt_eval_duration': 438269147, 'eval_count': 8, 'eval_duration': 233468572, 'message': Message(role='assistant', content='The capital of France is Paris.', images=None, tool_calls=None), 'usage': {'prompt_tokens': 32, 'completion_tokens': 8, 'total_tokens': 40}}, delta=None, logprobs=None, additional_kwargs={})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "llm.chat([ChatMessage(role=\"user\", content=\"What is the capital of France?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a: int, b: int) -> int:\n",
    "    return a + b\n",
    "\n",
    "def sub(a: int, b: int) -> int:\n",
    "    return a - b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResponse(message=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text=\"Hi! I'm doing well, thanks for asking. It's great to chat with you. How about you? What's new and exciting in your world?\")]), raw=ModelResponse(id='chatcmpl-b04b8dfe-39a5-4313-90d5-5f0242bd9661', created=1744386869, model='ollama/llama3:latest', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Hi! I'm doing well, thanks for asking. It's great to chat with you. How about you? What's new and exciting in your world?\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=33, prompt_tokens=10, total_tokens=43, completion_tokens_details=None, prompt_tokens_details=None)), delta=None, logprobs=None, additional_kwargs={})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
